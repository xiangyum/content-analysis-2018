{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 - Information Extraction\n",
    "\n",
    "\n",
    "This week, we move from arbitrary textual classification to the use of computation and linguistic models to parse precise claims from documents. Rather than focusing on simply the *ideas* in a corpus, here we focus on understanding and extracting its precise *claims*. This process involves a sequential pipeline of classifying and structuring tokens from text, each of which generates potentially useful data for the content analyst. Steps in this process, which we examine in this notebook, include: 1) tagging words by their part of speech (POS) to reveal the linguistic role they play in the sentence (e.g., Verb, Noun, Adjective, etc.); 2) tagging words as named entities (NER) such as places or organizations; 3) structuring or \"parsing\" sentences into nested phrases that are local to, describe or depend on one another; and 4) extracting informational claims from those phrases, like the Subject-Verb-Object (SVO) triples we extract here. While much of this can be done directly in the python package NLTK that we introduced in week 2, here we use NLTK bindings to the Stanford NLP group's open software, written in Java. Try typing a sentence into the online version [here](http://nlp.stanford.edu:8080/corenlp/) to get a sense of its potential. It is superior in performance to NLTK's implementations, but takes time to run, and so for these exercises we will parse and extract information for a very small text corpus. Of course, for final projects that draw on these tools, we encourage you to install the software on your own machines or shared servers at the university (RCC, SSRC) in order to perform these operations on much more text. \n",
    "\n",
    "For this notebook we will be using the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "#Make sure you update it before starting this notebook\n",
    "import lucem_illud #pip install -U git+git://github.com/Computational-Content-Analysis-2018/lucem_illud.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "#For NLP\n",
    "import nltk\n",
    "\n",
    "import numpy as np #For arrays\n",
    "import pandas #Gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "\n",
    "#Displays the graphs\n",
    "import graphviz #You also need to install the command line graphviz\n",
    "\n",
    "#These are from the standard library\n",
    "import os.path\n",
    "import zipfile\n",
    "import subprocess\n",
    "import io\n",
    "import tempfile\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to run this _once_ to download everything, you will also need [Java 1.8+](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html) if you are using Windows or MacOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting downloads, this will take 5-10 minutes\n",
      "../stanford-NLP/parser already exists, skipping download\n",
      "../stanford-NLP/ner already exists, skipping download\n",
      "../stanford-NLP/postagger already exists, skipping download\n",
      "../stanford-NLP/core already exists, skipping download\n",
      "Done setting up the Stanford NLP collection\n"
     ]
    }
   ],
   "source": [
    "lucem_illud.setupStanfordNLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to have stanford-NLP setup before importing, so we are doing the import here. IF you have stanford-NLP working, you can import at the beginning like you would with any other library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiangyu/anaconda3/lib/python3.6/site-packages/nltk/tag/stanford.py:183: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n",
      "/Users/xiangyu/anaconda3/lib/python3.6/site-packages/nltk/tag/stanford.py:149: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import lucem_illud.stanford as stanford"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Information Extraction is a module packaged within the Stanford Core NLP package, but it is not yet supported by `nltk`. As a result, we have defining our own `lucem_illud` function that runs the Stanford Core NLP java code right here. For other projects, it is often useful to use Java or other programs (in C, C++) within a python workflow, and this is an example. `stanford.openIE()` takes in a string or list of strings and then produces as output all the subject, verb, object (SVO) triples Stanford Corenlp can find, as a DataFrame. You can do this through links to the Stanford Core NLP project that we provide here, or play with their interface directly (in the penultimate cell of this notebook), which produces data in \"pretty graphics\" like this example parsing of the first sentence in the \"Shooting of Trayvon Martin\" Wikipedia article:\n",
    "\n",
    "![Output 1](../data/stanford_core1.png)\n",
    "![Output 2](../data/stanford_core2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will illustrate these tools on some *very* short examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw the elephant in my pajamas.\n",
      "The quick brown fox jumped over the lazy dog.\n",
      "While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.\n",
      "Trayvon Benjamin Martin was an African American from Miami Gardens, Florida, who, at 17 years old, was fatally shot by George Zimmerman, a neighborhood watch volunteer, in Sanford, Florida.\n",
      "Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo\n"
     ]
    }
   ],
   "source": [
    "text = ['I saw the elephant in my pajamas.', 'The quick brown fox jumped over the lazy dog.', 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.', 'Trayvon Benjamin Martin was an African American from Miami Gardens, Florida, who, at 17 years old, was fatally shot by George Zimmerman, a neighborhood watch volunteer, in Sanford, Florida.', 'Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo']\n",
    "tokenized_text = [nltk.word_tokenize(t) for t in text]\n",
    "print('\\n'.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-Speech (POS) tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In POS tagging, we classify each word by its semantic role in a sentence. The Stanford POS tagger uses the [Penn Treebank tag set]('http://repository.upenn.edu/cgi/viewcontent.cgi?article=1603&context=cis_reports') to POS tag words from input sentences. As discussed in the second assignment, this is a relatively precise tagset, which allows more informative tags, and also more opportunities to err :-).\n",
    "\n",
    "|#. |Tag |Description |\n",
    "|---|----|------------|\n",
    "|1.\t|CC\t|Coordinating conjunction\n",
    "|2.\t|CD\t|Cardinal number\n",
    "|3.\t|DT\t|Determiner\n",
    "|4.\t|EX\t|Existential there\n",
    "|5.\t|FW\t|Foreign word\n",
    "|6.\t|IN\t|Preposition or subordinating conjunction\n",
    "|7.\t|JJ\t|Adjective\n",
    "|8.\t|JJR|\tAdjective, comparative\n",
    "|9.\t|JJS|\tAdjective, superlative\n",
    "|10.|\tLS\t|List item marker\n",
    "|11.|\tMD\t|Modal\n",
    "|12.|\tNN\t|Noun, singular or mass\n",
    "|13.|\tNNS\t|Noun, plural\n",
    "|14.|\tNNP\t|Proper noun, singular\n",
    "|15.|\tNNPS|\tProper noun, plural\n",
    "|16.|\tPDT\t|Predeterminer\n",
    "|17.|\tPOS\t|Possessive ending\n",
    "|18.|\tPRP\t|Personal pronoun\n",
    "|19.|\tPRP\\$|\tPossessive pronoun\n",
    "|20.|\tRB\t|Adverb\n",
    "|21.|\tRBR\t|Adverb, comparative\n",
    "|22.|\tRBS\t|Adverb, superlative\n",
    "|23.|\tRP\t|Particle\n",
    "|24.|\tSYM\t|Symbol\n",
    "|25.|\tTO\t|to\n",
    "|26.|\tUH\t|Interjection\n",
    "|27.|\tVB\t|Verb, base form\n",
    "|28.|\tVBD\t|Verb, past tense\n",
    "|29.|\tVBG\t|Verb, gerund or present participle\n",
    "|30.|\tVBN\t|Verb, past participle\n",
    "|31.|\tVBP\t|Verb, non-3rd person singular present\n",
    "|32.|\tVBZ\t|Verb, 3rd person singular present\n",
    "|33.|\tWDT\t|Wh-determiner\n",
    "|34.|\tWP\t|Wh-pronoun\n",
    "|35.|\tWP$\t|Possessive wh-pronoun\n",
    "|36.|\tWRB\t|Wh-adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('I', 'PRP'), ('saw', 'VBD'), ('the', 'DT'), ('elephant', 'NN'), ('in', 'IN'), ('my', 'PRP$'), ('pajamas', 'NNS'), ('.', '.')], [('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'), ('jumped', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')], [('While', 'IN'), ('in', 'IN'), ('France', 'NNP'), (',', ','), ('Christine', 'NNP'), ('Lagarde', 'NNP'), ('discussed', 'VBD'), ('short-term', 'JJ'), ('stimulus', 'NN'), ('efforts', 'NNS'), ('in', 'IN'), ('a', 'DT'), ('recent', 'JJ'), ('interview', 'NN'), ('with', 'IN'), ('the', 'DT'), ('Wall', 'NNP'), ('Street', 'NNP'), ('Journal', 'NNP'), ('.', '.')], [('Trayvon', 'NNP'), ('Benjamin', 'NNP'), ('Martin', 'NNP'), ('was', 'VBD'), ('an', 'DT'), ('African', 'NNP'), ('American', 'NNP'), ('from', 'IN'), ('Miami', 'NNP'), ('Gardens', 'NNP'), (',', ','), ('Florida', 'NNP'), (',', ','), ('who', 'WP'), (',', ','), ('at', 'IN'), ('17', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('was', 'VBD'), ('fatally', 'RB'), ('shot', 'VBN'), ('by', 'IN'), ('George', 'NNP'), ('Zimmerman', 'NNP'), (',', ','), ('a', 'DT'), ('neighborhood', 'NN'), ('watch', 'NN'), ('volunteer', 'NN'), (',', ','), ('in', 'IN'), ('Sanford', 'NNP'), (',', ','), ('Florida', 'NNP'), ('.', '.')], [('Buffalo', 'NNP'), ('buffalo', 'NN'), ('Buffalo', 'NNP'), ('buffalo', 'NN'), ('buffalo', 'NN'), ('buffalo', 'NN'), ('Buffalo', 'NNP'), ('buffalo', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "pos_sents = stanford.postTagger.tag_sents(tokenized_text)\n",
    "print(pos_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks quite good. Now we will try POS tagging with a somewhat larger corpus. We consider a few of the top posts from the reddit data we used last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redditDF = pandas.read_csv('../data/reddit.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grabbing the 10 highest scoring posts and tokenizing the sentences. Once again, notice that we aren't going to do any kind of stemming this week (although *semantic* normalization may be performed where we translate synonyms into the same focal word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>over_18</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>goldie-gold</td>\n",
       "      <td>False</td>\n",
       "      <td>12650</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>This just happened...  So, I had a laptop syst...</td>\n",
       "      <td>Engineer is doing drugs!! No. No they aren't.</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[This, just, happened, ...], [So, ,, I, had, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TheDroolinFool</td>\n",
       "      <td>False</td>\n",
       "      <td>13152</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>Another tale from the out of hours IT desk... ...</td>\n",
       "      <td>\"I need you to fix Google Bing immediately!\"</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[Another, tale, from, the, out, of, hours, IT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clickity_clickity</td>\n",
       "      <td>False</td>\n",
       "      <td>13404</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>[Part 1](http://www.reddit.com/r/talesfromtech...</td>\n",
       "      <td>Jack, the Worst End User, Part 4</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[[, Part, 1, ], (, http, :, //www.reddit.com/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SECGaz</td>\n",
       "      <td>False</td>\n",
       "      <td>13724</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>&gt; $Me  - Hello, IT.   &gt; $Usr - Hi, I am still ...</td>\n",
       "      <td>Hi, I am still off sick but I am not.</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[&gt;, $, Me, -, Hello, ,, IT, .], [&gt;, $, Usr, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>guitarsdontdance</td>\n",
       "      <td>False</td>\n",
       "      <td>14089</td>\n",
       "      <td>Tales From Tech Support</td>\n",
       "      <td>So my story starts on what was a normal day ta...</td>\n",
       "      <td>\"Don't bother sending a tech, I'll be dead by ...</td>\n",
       "      <td>https://www.reddit.com/r/talesfromtechsupport/...</td>\n",
       "      <td>[[So, my, story, starts, on, what, was, a, nor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              author  over_18  score                subreddit  \\\n",
       "4        goldie-gold    False  12650  Tales From Tech Support   \n",
       "3     TheDroolinFool    False  13152  Tales From Tech Support   \n",
       "2  Clickity_clickity    False  13404  Tales From Tech Support   \n",
       "1             SECGaz    False  13724  Tales From Tech Support   \n",
       "0   guitarsdontdance    False  14089  Tales From Tech Support   \n",
       "\n",
       "                                                text  \\\n",
       "4  This just happened...  So, I had a laptop syst...   \n",
       "3  Another tale from the out of hours IT desk... ...   \n",
       "2  [Part 1](http://www.reddit.com/r/talesfromtech...   \n",
       "1  > $Me  - Hello, IT.   > $Usr - Hi, I am still ...   \n",
       "0  So my story starts on what was a normal day ta...   \n",
       "\n",
       "                                               title  \\\n",
       "4      Engineer is doing drugs!! No. No they aren't.   \n",
       "3       \"I need you to fix Google Bing immediately!\"   \n",
       "2                   Jack, the Worst End User, Part 4   \n",
       "1              Hi, I am still off sick but I am not.   \n",
       "0  \"Don't bother sending a tech, I'll be dead by ...   \n",
       "\n",
       "                                                 url  \\\n",
       "4  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "3  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "2  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "1  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "0  https://www.reddit.com/r/talesfromtechsupport/...   \n",
       "\n",
       "                                           sentences  \n",
       "4  [[This, just, happened, ...], [So, ,, I, had, ...  \n",
       "3  [[Another, tale, from, the, out, of, hours, IT...  \n",
       "2  [[[, Part, 1, ], (, http, :, //www.reddit.com/...  \n",
       "1  [[>, $, Me, -, Hello, ,, IT, .], [>, $, Usr, -...  \n",
       "0  [[So, my, story, starts, on, what, was, a, nor...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redditTopScores = redditDF.sort_values('score')[-10:]\n",
    "redditTopScores['sentences'] = redditTopScores['text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "redditTopScores.index = range(len(redditTopScores) - 1, -1,-1) #Reindex to make things nice in the future\n",
    "redditTopScores[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redditTopScores['POS_sents'] = redditTopScores['sentences'].apply(lambda x: stanford.postTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9    [[(Last, JJ), (year, NN), (,, ,), (Help, NN), ...\n",
       "8    [[(First, JJ), (post, NN), (in, IN), (quite, R...\n",
       "7    [[([, NNP), (Original, NNP), (Post, NNP), (], ...\n",
       "6    [[(I, PRP), (witnessed, VBD), (this, DT), (ast...\n",
       "5    [[(I, PRP), (work, VBP), (Helpdesk, NNP), (for...\n",
       "4    [[(This, DT), (just, RB), (happened, VBN), (.....\n",
       "3    [[(Another, DT), (tale, NN), (from, IN), (the,...\n",
       "2    [[([, NNP), (Part, NNP), (1, CD), (], FW), ((,...\n",
       "1    [[(>, JJR), ($, $), (Me, PRP), (-, :), (Hello,...\n",
       "0    [[(So, RB), (my, PRP$), (story, NN), (starts, ...\n",
       "Name: POS_sents, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redditTopScores['POS_sents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And count the number of `NN` (nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('password', 21),\n",
       " ('(', 19),\n",
       " ('time', 14),\n",
       " (')', 14),\n",
       " ('lot', 12),\n",
       " ('computer', 12),\n",
       " ('life', 11),\n",
       " ('email', 11),\n",
       " ('**Genius**', 10),\n",
       " ('message', 9),\n",
       " ('**Me**', 9),\n",
       " ('system', 9),\n",
       " ('day', 9),\n",
       " ('call', 8),\n",
       " ('laptop', 8),\n",
       " ('office', 8),\n",
       " ('part', 8),\n",
       " ('today', 8),\n",
       " ('story', 8),\n",
       " ('user', 7)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'NN'\n",
    "targetCounts = {}\n",
    "for entry in redditTopScores['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the number of top verbs (`VB`)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 18),\n",
       " ('have', 17),\n",
       " ('get', 14),\n",
       " ('do', 11),\n",
       " ('change', 9),\n",
       " ('make', 8),\n",
       " ('know', 7),\n",
       " ('say', 7),\n",
       " ('help', 6),\n",
       " ('look', 6),\n",
       " ('tell', 6),\n",
       " ('send', 6),\n",
       " ('go', 5),\n",
       " ('work', 4),\n",
       " ('use', 4),\n",
       " ('receive', 4),\n",
       " ('thank', 4),\n",
       " ('feel', 4),\n",
       " ('want', 4),\n",
       " ('call', 4)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countTarget = 'VB'\n",
    "targetCounts = {}\n",
    "for entry in redditTopScores['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the adjectives that modify the word, \"computer\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'own', 'unrestricted'}\n"
     ]
    }
   ],
   "source": [
    "NTarget = 'JJ'\n",
    "Word = 'computer'\n",
    "NResults = set()\n",
    "for entry in redditTopScores['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for (ent1, kind1),(ent2,kind2) in zip(sentence[:-1], sentence[1:]):\n",
    "            if (kind1,ent2.lower())==(NTarget,Word):\n",
    "                NResults.add(ent1)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "print(NResults)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating POS tagger\n",
    "\n",
    "We can check the POS tagger by running it on a manually tagged corpus and identifying a reasonable error metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " (',', ','),\n",
       " ('61', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " (',', ','),\n",
       " ('will', 'MD'),\n",
       " ('join', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('nonexecutive', 'JJ'),\n",
       " ('director', 'NN'),\n",
       " ('Nov.', 'NNP'),\n",
       " ('29', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeBank = nltk.corpus.treebank\n",
    "treeBank.tagged_sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeBank.sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stanfordTags = stanford.postTagger.tag_sents(treeBank.sents()[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And compare the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: Dutch  \tStanford: JJ\tTreebank: NNP\n",
      "Word: publishing  \tStanford: NN\tTreebank: VBG\n",
      "Word: used  \tStanford: VBD\tTreebank: VBN\n",
      "Word: more  \tStanford: JJR\tTreebank: RBR\n",
      "Word: ago  \tStanford: RB\tTreebank: IN\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: later  \tStanford: RB\tTreebank: JJ\n",
      "Word: New  \tStanford: NNP\tTreebank: JJ\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: more  \tStanford: JJR\tTreebank: RBR\n",
      "Word: ago  \tStanford: RB\tTreebank: IN\n",
      "Word: ago  \tStanford: RB\tTreebank: IN\n",
      "Word: replaced  \tStanford: VBD\tTreebank: VBN\n",
      "Word: more  \tStanford: JJR\tTreebank: JJ\n",
      "Word: expected  \tStanford: VBD\tTreebank: VBN\n",
      "Word: study  \tStanford: VBD\tTreebank: VBP\n",
      "Word: studied  \tStanford: VBD\tTreebank: VBN\n",
      "Word: industrialized  \tStanford: JJ\tTreebank: VBN\n",
      "Word: Lorillard  \tStanford: NNP\tTreebank: NN\n",
      "Word: found  \tStanford: VBD\tTreebank: VBN\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: rejected  \tStanford: VBD\tTreebank: VBN\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "Word: poured  \tStanford: VBN\tTreebank: VBD\n",
      "Word: in  \tStanford: IN\tTreebank: RP\n",
      "Word: that  \tStanford: IN\tTreebank: WDT\n",
      "The Precision is 96.547%\n"
     ]
    }
   ],
   "source": [
    "NumDiffs = 0\n",
    "for sentIndex in range(len(stanfordTags)):\n",
    "    for wordIndex in range(len(stanfordTags[sentIndex])):\n",
    "        if stanfordTags[sentIndex][wordIndex][1] != treeBank.tagged_sents()[sentIndex][wordIndex][1]:\n",
    "            if treeBank.tagged_sents()[sentIndex][wordIndex][1] != '-NONE-':\n",
    "                print(\"Word: {}  \\tStanford: {}\\tTreebank: {}\".format(stanfordTags[sentIndex][wordIndex][0], stanfordTags[sentIndex][wordIndex][1], treeBank.tagged_sents()[sentIndex][wordIndex][1]))\n",
    "                NumDiffs += 1\n",
    "total = sum([len(s) for s in stanfordTags])\n",
    "print(\"The Precision is {:.3f}%\".format((total-NumDiffs)/total * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that the stanford POS tagger is quite good. Nevertheless, for a 20 word sentence, we only have a 66% chance ($1-.96^{20}$) of tagging (and later parsing) it correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 1*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform POS tagging on a meaningful (but modest) subset of a corpus associated with your final project. Examine the list of words associated with at least three different parts of speech. Consider conditional frequencies (e.g., adjectives associated with nouns of interest or adverbs with verbs of interest). What do these distributions suggest about your corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the data & label the data\n",
    "stbtDF = pandas.read_csv(\"meritSTBT.csv\")\n",
    "stbtDF['sentences'] = stbtDF['text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "tocDF = pandas.read_excel('meritTOC.xlsx')\n",
    "tocDF['sentences'] = tocDF['text'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "#produce combined df\n",
    "combDF = pandas.concat([stbtDF,tocDF])\n",
    "combDF = combDF.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset = combDF[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiangyu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "subset['POS_sents'] = subset['sentences'].apply(lambda x: stanford.postTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('meritocracy', 41),\n",
       " ('cent', 35),\n",
       " ('society', 31),\n",
       " ('system', 28),\n",
       " ('year', 22),\n",
       " ('life', 22),\n",
       " ('time', 20),\n",
       " ('work', 19),\n",
       " ('family', 19),\n",
       " ('country', 19),\n",
       " ('education', 17),\n",
       " ('gap', 17),\n",
       " ('survey', 16),\n",
       " ('corruption', 16),\n",
       " (\"'I\", 15),\n",
       " (')', 14),\n",
       " ('school', 14),\n",
       " ('age', 13),\n",
       " ('world', 13),\n",
       " ('living', 13)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what are the top nouns?\n",
    "countTarget = 'NN'\n",
    "targetCounts = {}\n",
    "for entry in subset['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('other', 36),\n",
       " ('such', 33),\n",
       " ('social', 29),\n",
       " ('many', 26),\n",
       " ('young', 25),\n",
       " ('last', 23),\n",
       " ('good', 20),\n",
       " ('first', 19),\n",
       " ('economic', 19),\n",
       " ('different', 19),\n",
       " ('new', 16),\n",
       " ('national', 15),\n",
       " ('ST', 15),\n",
       " ('Singaporean', 15),\n",
       " ('public', 14),\n",
       " ('mental', 13),\n",
       " ('high', 13),\n",
       " ('foreign', 13),\n",
       " ('aged', 12),\n",
       " ('hard', 11)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what are the top adjectives?\n",
    "countTarget = 'JJ'\n",
    "targetCounts = {}\n",
    "for entry in subset['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 138),\n",
       " ('have', 47),\n",
       " ('do', 23),\n",
       " ('make', 16),\n",
       " ('help', 15),\n",
       " ('get', 14),\n",
       " ('take', 13),\n",
       " ('find', 12),\n",
       " ('work', 10),\n",
       " ('keep', 10),\n",
       " ('become', 9),\n",
       " ('go', 9),\n",
       " ('give', 7),\n",
       " ('think', 6),\n",
       " ('let', 6),\n",
       " ('want', 6),\n",
       " ('look', 6),\n",
       " ('see', 6),\n",
       " ('say', 6),\n",
       " ('succeed', 6)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what are the top verbs?\n",
    "countTarget = 'VB'\n",
    "targetCounts = {}\n",
    "for entry in subset['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('(', 4),\n",
       " ('sph.com.sg', 4),\n",
       " ('ongyiern', 2),\n",
       " ('IMH', 1),\n",
       " (')', 1),\n",
       " (\"'The\", 1),\n",
       " ('expatriate', 1),\n",
       " (\"'Zi\", 1),\n",
       " ('yu', 1),\n",
       " ('yang', 1),\n",
       " ('er', 1),\n",
       " ('qin', 1),\n",
       " ('bu', 1),\n",
       " ('zai', 1),\n",
       " ('en', 1),\n",
       " ('masse', 1),\n",
       " ('PMET', 1),\n",
       " ('a', 1),\n",
       " (\"'healthy\", 1),\n",
       " ('Roti', 1)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what are the foreign words?\n",
    "countTarget = 'FW'\n",
    "targetCounts = {}\n",
    "for entry in subset['POS_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != countTarget:\n",
    "                continue\n",
    "            elif ent in targetCounts:\n",
    "                targetCounts[ent] += 1\n",
    "            else:\n",
    "                targetCounts[ent] = 1\n",
    "sortedTargets = sorted(targetCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedTargets[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write a helper function for word associations\n",
    "def posword_assoc(xWord, yPos, dFrame):\n",
    "    NTarget = yPos\n",
    "    Word = xWord\n",
    "    NResults = set()\n",
    "    for entry in dFrame['POS_sents']:\n",
    "        for sentence in entry:\n",
    "            for (ent1, kind1),(ent2,kind2) in zip(sentence[:-1], sentence[1:]):\n",
    "                if (kind1,ent2.lower())==(NTarget,Word):\n",
    "                    NResults.add(ent1)\n",
    "                else:\n",
    "                    continue\n",
    "    print(NResults)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Extreme', 'Singaporean', 'multi-religious', 'extreme'}\n"
     ]
    }
   ],
   "source": [
    "posword_assoc('meritocracy', 'JJ', subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'maturing', 'fast-paced', 'ageing', 'just', 'stratified', 'modern', 'Civil', 'fast-changing', 'Chinese', 'democratic', 'multiracial', 'rugged', 'other', 'agrarian'}\n"
     ]
    }
   ],
   "source": [
    "posword_assoc('society', 'JJ', subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "posword_assoc('many', 'NN', subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'then'}\n"
     ]
    }
   ],
   "source": [
    "posword_assoc('education', 'RB', subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named-Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) is also a classification task, which identifies named objects. Included with Stanford NER are a 4 class model trained on the CoNLL 2003 eng.train, a 7 class model trained on the MUC 6 and MUC 7 training data sets, and a 3 class model trained on both data sets plus some additional data (including ACE 2002 and limited data in-house) on the intersection of those class sets. \n",
    "\n",
    "**3 class**:\tLocation, Person, Organization\n",
    "\n",
    "**4 class**:\tLocation, Person, Organization, Misc\n",
    "\n",
    "**7 class**:\tLocation, Person, Organization, Money, Percent, Date, Time\n",
    "\n",
    "These models each use distributional similarity features, which provide some performance gain at the cost of increasing their size and runtime. Also available are the same models missing those features.\n",
    "\n",
    "(We note that the training data for the 3 class model does not include any material from the CoNLL eng.testa or eng.testb data sets, nor any of the MUC 6 or 7 test or devtest datasets, nor Alan Ritter's Twitter NER data, so all of these would be valid tests of its performance.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we tag our first set of exemplary sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('I', 'O'), ('saw', 'O'), ('the', 'O'), ('elephant', 'O'), ('in', 'O'), ('my', 'O'), ('pajamas', 'O'), ('.', 'O')], [('The', 'O'), ('quick', 'O'), ('brown', 'O'), ('fox', 'O'), ('jumped', 'O'), ('over', 'O'), ('the', 'O'), ('lazy', 'O'), ('dog', 'O'), ('.', 'O')], [('While', 'O'), ('in', 'O'), ('France', 'LOCATION'), (',', 'O'), ('Christine', 'PERSON'), ('Lagarde', 'PERSON'), ('discussed', 'O'), ('short-term', 'O'), ('stimulus', 'O'), ('efforts', 'O'), ('in', 'O'), ('a', 'O'), ('recent', 'O'), ('interview', 'O'), ('with', 'O'), ('the', 'O'), ('Wall', 'ORGANIZATION'), ('Street', 'ORGANIZATION'), ('Journal', 'ORGANIZATION'), ('.', 'O')], [('Trayvon', 'PERSON'), ('Benjamin', 'PERSON'), ('Martin', 'PERSON'), ('was', 'O'), ('an', 'O'), ('African', 'O'), ('American', 'O'), ('from', 'O'), ('Miami', 'LOCATION'), ('Gardens', 'LOCATION'), (',', 'O'), ('Florida', 'LOCATION'), (',', 'O'), ('who', 'O'), (',', 'O'), ('at', 'O'), ('17', 'O'), ('years', 'O'), ('old', 'O'), (',', 'O'), ('was', 'O'), ('fatally', 'O'), ('shot', 'O'), ('by', 'O'), ('George', 'PERSON'), ('Zimmerman', 'PERSON'), (',', 'O'), ('a', 'O'), ('neighborhood', 'O'), ('watch', 'O'), ('volunteer', 'O'), (',', 'O'), ('in', 'O'), ('Sanford', 'LOCATION'), (',', 'O'), ('Florida', 'LOCATION'), ('.', 'O')], [('Buffalo', 'LOCATION'), ('buffalo', 'O'), ('Buffalo', 'ORGANIZATION'), ('buffalo', 'O'), ('buffalo', 'O'), ('buffalo', 'O'), ('Buffalo', 'ORGANIZATION'), ('buffalo', 'O')]]\n"
     ]
    }
   ],
   "source": [
    "classified_sents = stanford.nerTagger.tag_sents(tokenized_text)\n",
    "print(classified_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run NER over our entire corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redditTopScores['classified_sents'] = redditTopScores['sentences'].apply(lambda x: stanford.nerTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9    [[(Last, O), (year, O), (,, O), (Help, O), (De...\n",
       "8    [[(First, O), (post, O), (in, O), (quite, O), ...\n",
       "7    [[([, O), (Original, O), (Post, O), (], O), ((...\n",
       "6    [[(I, O), (witnessed, O), (this, O), (astoundi...\n",
       "5    [[(I, O), (work, O), (Helpdesk, ORGANIZATION),...\n",
       "4    [[(This, O), (just, O), (happened, O), (..., O...\n",
       "3    [[(Another, O), (tale, O), (from, O), (the, O)...\n",
       "2    [[([, O), (Part, O), (1, O), (], O), ((, O), (...\n",
       "1    [[(>, O), ($, O), (Me, O), (-, O), (Hello, O),...\n",
       "0    [[(So, O), (my, O), (story, O), (starts, O), (...\n",
       "Name: classified_sents, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redditTopScores['classified_sents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most common entities (which are, of course, boring):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 401),\n",
       " ('I', 245),\n",
       " ('the', 226),\n",
       " (',', 205),\n",
       " ('to', 197),\n",
       " ('a', 143),\n",
       " ('and', 135),\n",
       " ('>', 106),\n",
       " ('you', 102),\n",
       " ('of', 97)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entityCounts = {}\n",
    "for entry in redditTopScores['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if ent in entityCounts:\n",
    "                entityCounts[ent] += 1\n",
    "            else:\n",
    "                entityCounts[ent] = 1\n",
    "sortedEntities = sorted(entityCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedEntities[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or those occurring only twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year',\n",
       " 'Desk',\n",
       " 'busy',\n",
       " 'fix',\n",
       " 'received',\n",
       " 'couple',\n",
       " 'Windows',\n",
       " 'anymore',\n",
       " 'Sure',\n",
       " 'error',\n",
       " 'DVD',\n",
       " 'opened',\n",
       " 'There',\n",
       " 'upside',\n",
       " 'local',\n",
       " 'bane',\n",
       " 'existence',\n",
       " 'learn',\n",
       " 'sometimes',\n",
       " 'generic',\n",
       " 'Everyone',\n",
       " 'login',\n",
       " 'times',\n",
       " 'guy',\n",
       " 'asset',\n",
       " 'name',\n",
       " 'Computer',\n",
       " 'nothing',\n",
       " \"'P4ssword\",\n",
       " 'P',\n",
       " 'Everything',\n",
       " 'case',\n",
       " '*type',\n",
       " 'S',\n",
       " 'LOWERCASE',\n",
       " 'used',\n",
       " 'four',\n",
       " 'Original',\n",
       " 'cancer',\n",
       " 'month',\n",
       " 'live',\n",
       " 'brave',\n",
       " 'bitter',\n",
       " 'passed',\n",
       " 'ago',\n",
       " 'absolutely',\n",
       " 'ready',\n",
       " 'proud',\n",
       " 'above',\n",
       " 'completely',\n",
       " 'its',\n",
       " 'meant',\n",
       " 'both',\n",
       " 'sharing',\n",
       " 'making',\n",
       " '100',\n",
       " 'share',\n",
       " 'looking',\n",
       " 'ALL',\n",
       " 'whom',\n",
       " 'business',\n",
       " 'whose',\n",
       " 'stronger',\n",
       " 'bad',\n",
       " 'mess',\n",
       " 'turn',\n",
       " 'first',\n",
       " 'others',\n",
       " 'Here',\n",
       " 'suggested',\n",
       " 'videos',\n",
       " 'While',\n",
       " 'stand',\n",
       " 'certain',\n",
       " 'enjoy',\n",
       " 'well',\n",
       " 'drowned',\n",
       " 'soon',\n",
       " 'understand',\n",
       " 'risks',\n",
       " 'myself',\n",
       " 'point',\n",
       " 'future',\n",
       " 'avoid',\n",
       " 'thinking',\n",
       " 'information',\n",
       " 'insurance',\n",
       " 'site',\n",
       " 'step',\n",
       " 'guide',\n",
       " 'discover',\n",
       " 'order',\n",
       " '5',\n",
       " 'slightly',\n",
       " 'spent',\n",
       " 'moment',\n",
       " 'arms',\n",
       " 'idea',\n",
       " 'food',\n",
       " 'party',\n",
       " 'played',\n",
       " 'family',\n",
       " 'allowed',\n",
       " 'cry',\n",
       " 'pretty',\n",
       " 'nice',\n",
       " 'loved',\n",
       " 'mind',\n",
       " 'favor',\n",
       " 'watched',\n",
       " 'Things',\n",
       " '17',\n",
       " 'small',\n",
       " 'lived',\n",
       " 'living',\n",
       " 'themselves',\n",
       " 'potential',\n",
       " 'happiness',\n",
       " 'sound',\n",
       " 'situation',\n",
       " 'believe',\n",
       " 'mistakes',\n",
       " 'same',\n",
       " 'scenario',\n",
       " 'difference',\n",
       " 'glad',\n",
       " 'flaws',\n",
       " 'stupid',\n",
       " 'yourself',\n",
       " 'ok',\n",
       " 'In',\n",
       " 'comments',\n",
       " 'SO',\n",
       " 'random',\n",
       " 'request',\n",
       " 'Give',\n",
       " 'THIS',\n",
       " 'response',\n",
       " 'Thanks',\n",
       " 'tears',\n",
       " 'helped',\n",
       " 'reply',\n",
       " 'large',\n",
       " 'academic',\n",
       " 'organization',\n",
       " 'CEO',\n",
       " 'Of',\n",
       " 'course',\n",
       " 'mailboxes',\n",
       " 'Fail',\n",
       " '#',\n",
       " 'check',\n",
       " 'generate',\n",
       " '=',\n",
       " 'real',\n",
       " 'stopped',\n",
       " 'avalanche',\n",
       " 'died',\n",
       " 'systems',\n",
       " 'staff',\n",
       " 'brought',\n",
       " 'retail',\n",
       " 'store',\n",
       " 'plugged',\n",
       " 'hear',\n",
       " 'occasionally',\n",
       " 'operate',\n",
       " 'drawer*',\n",
       " 'try',\n",
       " 'hit',\n",
       " '*I',\n",
       " 'echo',\n",
       " 'heard',\n",
       " 'seconds',\n",
       " 'nose',\n",
       " 'working',\n",
       " 'BING',\n",
       " 'THE',\n",
       " '*Note',\n",
       " 'yes',\n",
       " 'different',\n",
       " 'search',\n",
       " 'immediately',\n",
       " 'connection',\n",
       " 'Turns',\n",
       " 'shortcut',\n",
       " 'okay',\n",
       " 'computering',\n",
       " 'taking',\n",
       " 'Steve',\n",
       " 'XYZ',\n",
       " 'however',\n",
       " 'forwarded',\n",
       " 'using',\n",
       " 'meet',\n",
       " 'ran',\n",
       " 'mouse',\n",
       " 'closed',\n",
       " 'window',\n",
       " 'revealing',\n",
       " 'me*',\n",
       " 'shaking',\n",
       " 'lunch',\n",
       " 'yelled',\n",
       " 'needed',\n",
       " 'key',\n",
       " 'week',\n",
       " 'pointed',\n",
       " 'door',\n",
       " 'blinked',\n",
       " 'Then',\n",
       " 'command',\n",
       " 'three',\n",
       " 'web',\n",
       " 'pages',\n",
       " 'person',\n",
       " 'Whatever',\n",
       " 'um',\n",
       " 'wrong',\n",
       " 'mother',\n",
       " 'done',\n",
       " 'way',\n",
       " 'weeks',\n",
       " 'since',\n",
       " 'stuff',\n",
       " 'took',\n",
       " 'Wow',\n",
       " 'gildings',\n",
       " 'One',\n",
       " 'HR',\n",
       " 'select',\n",
       " '*Are',\n",
       " 'Ca',\n",
       " 'building',\n",
       " 'holiday',\n",
       " 'pay',\n",
       " 'gods',\n",
       " 'expire',\n",
       " '60',\n",
       " 'anyway',\n",
       " 'User',\n",
       " 'DeskMugPhonePencil1',\n",
       " 'Thursday',\n",
       " 'return',\n",
       " 'calls',\n",
       " 'often',\n",
       " 'issues',\n",
       " 'service',\n",
       " 'older',\n",
       " 'types',\n",
       " 'speak',\n",
       " 'box',\n",
       " 'properly',\n",
       " 'personally',\n",
       " 'ask',\n",
       " 'supervisor',\n",
       " 'earlier',\n",
       " 'visit',\n",
       " 'terrible',\n",
       " 'willing',\n",
       " 'nasty']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0] for x in sortedEntities if x[1] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also list the most common \"non-objects\". (We note that we're not graphing these because there are so few here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jack', 17),\n",
       " ('Google', 6),\n",
       " ('Smith', 5),\n",
       " ('Steve', 2),\n",
       " ('Citrix', 1),\n",
       " ('Nono', 1),\n",
       " ('Reddit', 1),\n",
       " ('Helpdesk', 1),\n",
       " ('UK', 1),\n",
       " ('CMD', 1)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonObjCounts = {}\n",
    "for entry in redditTopScores['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind == 'O':\n",
    "                continue\n",
    "            elif ent in nonObjCounts:\n",
    "                nonObjCounts[ent] += 1\n",
    "            else:\n",
    "                nonObjCounts[ent] = 1\n",
    "sortedNonObj = sorted(nonObjCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedNonObj[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the Organizations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Google', 6), ('Citrix', 1), ('Helpdesk', 1), ('CMD', 1), ('GOOGLE', 1)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OrgCounts = {}\n",
    "for entry in redditTopScores['classified_sents']:\n",
    "    for sentence in entry:\n",
    "        for ent, kind in sentence:\n",
    "            if kind != 'ORGANIZATION':\n",
    "                continue\n",
    "            elif ent in OrgCounts:\n",
    "                OrgCounts[ent] += 1\n",
    "            else:\n",
    "                OrgCounts[ent] = 1\n",
    "sortedOrgs = sorted(OrgCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "sortedOrgs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These, of course, have much smaller counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 2*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform NER on a (modest) subset of your corpus of interest. List all of the different kinds of entities tagged? What does their distribution suggest about the focus of your corpus? For a subset of your corpus, tally at least one type of named entity and calculate the Precision, Recall and F-score for the NER classification just performed (using your own hand-codings as \"ground truth\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiangyu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#apply NER to sentences\n",
    "subset['classified_sents'] = subset['sentences'].apply(lambda x: stanford.nerTagger.tag_sents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def commonNre(xType, dFrame):\n",
    "    OrgCounts = {}\n",
    "    for entry in dFrame['classified_sents']:\n",
    "        for sentence in entry:\n",
    "            for ent, kind in sentence:\n",
    "                if kind != xType:\n",
    "                    continue\n",
    "                elif ent in OrgCounts:\n",
    "                    OrgCounts[ent] += 1\n",
    "                else:\n",
    "                    OrgCounts[ent] = 1\n",
    "    sortedOrgs = sorted(OrgCounts.items(), key = lambda x: x[1], reverse = True)\n",
    "    print(sortedOrgs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Lee', 34), ('Chong', 15), ('Kuo', 11), ('Jamiyah', 8), ('Mr', 7), ('Hsien', 7), ('Loong', 7), ('Ng', 6), ('Tan', 5), ('Eyan', 4)]\n"
     ]
    }
   ],
   "source": [
    "commonNre('PERSON', subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Singapore', 154), ('China', 9), ('India', 8), ('US', 7), ('Lanka', 6), ('Britain', 5), ('Sri', 5), ('United', 3), ('States', 3), ('New', 3)]\n"
     ]
    }
   ],
   "source": [
    "commonNre('LOCATION', subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PAP', 16), ('of', 13), ('Singapore', 12), ('National', 10), ('Board', 10), ('Health', 9), ('Times', 9), ('Straits', 8), ('University', 7), ('School', 6)]\n"
     ]
    }
   ],
   "source": [
    "commonNre('ORGANIZATION', subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing\n",
    "\n",
    "Here we will introduce the Stanford Parser by feeding it tokenized text from our initial example sentences. The parser is a dependency parser, but this initial program outputs a simple, self-explanatory phrase-structure representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('NNP', ['Trayvon']), Tree('NNP', ['Benjamin']), Tree('NNP', ['Martin'])]), Tree('VP', [Tree('VBD', ['was']), Tree('NP', [Tree('NP', [Tree('DT', ['an']), Tree('NNP', ['African']), Tree('NNP', ['American'])]), Tree('PP', [Tree('IN', ['from']), Tree('NP', [Tree('NP', [Tree('NNP', ['Miami']), Tree('NNPS', ['Gardens'])]), Tree(',', [',']), Tree('NP', [Tree('NNP', ['Florida'])]), Tree(',', [',']), Tree('SBAR', [Tree('WHNP', [Tree('WP', ['who'])]), Tree('S', [Tree(',', [',']), Tree('PP', [Tree('IN', ['at']), Tree('ADJP', [Tree('NP', [Tree('CD', ['17']), Tree('NNS', ['years'])]), Tree('JJ', ['old'])])]), Tree(',', [',']), Tree('VP', [Tree('VBD', ['was']), Tree('ADVP', [Tree('RB', ['fatally'])]), Tree('VP', [Tree('VBN', ['shot']), Tree('PP', [Tree('IN', ['by']), Tree('NP', [Tree('NP', [Tree('NNP', ['George']), Tree('NNP', ['Zimmerman'])]), Tree(',', [',']), Tree('NP', [Tree('DT', ['a']), Tree('NN', ['neighborhood']), Tree('NN', ['watch']), Tree('NN', ['volunteer'])]), Tree(',', [','])])]), Tree('PP', [Tree('IN', ['in']), Tree('NP', [Tree('NNP', ['Sanford']), Tree(',', [',']), Tree('NNP', ['Florida'])])])])])])])])])])]), Tree('.', ['.'])])])]\n"
     ]
    }
   ],
   "source": [
    "parses = list(stanford.parser.parse_sents(tokenized_text)) #Converting the iterator to a list so we can call by index. They are still \n",
    "fourthSentParseTree = list(parses[3]) #iterators so be careful about re-running code, without re-running this block\n",
    "print(fourthSentParseTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trees are a common data structure and there are a large number of things to do with them. What we are intetered in is the relationship between different types of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def treeRelation(parsetree, relationType, *targets):\n",
    "    if isinstance(parsetree, list):\n",
    "        parsetree = parsetree[0]\n",
    "    if set(targets) & set(parsetree.leaves()) != set(targets):\n",
    "        return []\n",
    "    else:\n",
    "        retList = []\n",
    "        for subT in parsetree.subtrees():\n",
    "            if subT.label() == relationType:\n",
    "                if set(targets) & set(subT.leaves()) == set(targets):\n",
    "                    retList.append([(subT.label(), ' '.join(subT.leaves()))])\n",
    "    return retList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def treeSubRelation(parsetree, relationTypeScope, relationTypeTarget, *targets):\n",
    "    if isinstance(parsetree, list):\n",
    "        parsetree = parsetree[0]\n",
    "    if set(targets) & set(parsetree.leaves()) != set(targets):\n",
    "        return []\n",
    "    else:\n",
    "        retSet = set()\n",
    "        for subT in parsetree.subtrees():\n",
    "            if set(targets) & set(subT.leaves()) == set(targets):\n",
    "                if subT.label() == relationTypeScope:\n",
    "                    for subsub in subT.subtrees():\n",
    "                        if subsub.label()==relationTypeTarget:\n",
    "                            retSet.add(' '.join(subsub.leaves()))\n",
    "    return retSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('NP',\n",
       "   'an African American from Miami Gardens , Florida , who , at 17 years old , was fatally shot by George Zimmerman , a neighborhood watch volunteer , in Sanford , Florida')],\n",
       " [('NP',\n",
       "   'Miami Gardens , Florida , who , at 17 years old , was fatally shot by George Zimmerman , a neighborhood watch volunteer , in Sanford , Florida')]]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeRelation(fourthSentParseTree, 'NP', 'Florida', 'who')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that Florida occurs twice in two different nested noun phrases in the sentence. \n",
    "\n",
    "We can also find all of the verbs within the noun phrase defined by one or more target words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shot'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treeSubRelation(fourthSentParseTree, 'NP', 'VBN', 'Florida', 'who')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if we want to to look at the whole tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                   ROOT                                                                                                                       \n",
      "                                                                                                                    |                                                                                                                          \n",
      "                                                                                                                    S                                                                                                                         \n",
      "            ________________________________________________________________________________________________________|_______________________________________________________________________________________________________________________   \n",
      "           |                       VP                                                                                                                                                                                                       | \n",
      "           |              _________|______________                                                                                                                                                                                          |  \n",
      "           |             |                        NP                                                                                                                                                                                        | \n",
      "           |             |          ______________|________________                                                                                                                                                                         |  \n",
      "           |             |         |                               PP                                                                                                                                                                       | \n",
      "           |             |         |               ________________|___________                                                                                                                                                             |  \n",
      "           |             |         |              |                            NP                                                                                                                                                           | \n",
      "           |             |         |              |           _________________|____________________________________                                                                                                                        |  \n",
      "           |             |         |              |          |           |     |     |                             SBAR                                                                                                                     | \n",
      "           |             |         |              |          |           |     |     |    __________________________|______________________________                                                                                         |  \n",
      "           |             |         |              |          |           |     |     |   |                                                         S                                                                                        | \n",
      "           |             |         |              |          |           |     |     |   |     ____________________________________________________|_______________________                                                                 |  \n",
      "           |             |         |              |          |           |     |     |   |    |           |              |                                                 VP                                                               | \n",
      "           |             |         |              |          |           |     |     |   |    |           |              |    _____________________________________________|_______                                                         |  \n",
      "           |             |         |              |          |           |     |     |   |    |           |              |   |     |                                               VP                                                       | \n",
      "           |             |         |              |          |           |     |     |   |    |           |              |   |     |      _________________________________________|________________________________________                |  \n",
      "           |             |         |              |          |           |     |     |   |    |           PP             |   |     |     |                      PP                                                          |               | \n",
      "           |             |         |              |          |           |     |     |   |    |    _______|____          |   |     |     |     _________________|__________                                                 |               |  \n",
      "           |             |         |              |          |           |     |     |   |    |   |           ADJP       |   |     |     |    |                            NP                                               PP              | \n",
      "           |             |         |              |          |           |     |     |   |    |   |        ____|____     |   |     |     |    |           _________________|________________________________     ___________|___            |  \n",
      "           NP            |         NP             |          NP          |     NP    |  WHNP  |   |       NP        |    |   |    ADVP   |    |          NP            |           NP                       |   |               NP          | \n",
      "    _______|_______      |    _____|_______       |      ____|_____      |     |     |   |    |   |    ___|____     |    |   |     |     |    |     _____|______       |    _______|_________________       |   |      _________|_____      |  \n",
      "  NNP     NNP     NNP   VBD  DT   NNP     NNP     IN   NNP        NNPS   ,    NNP    ,   WP   ,   IN  CD      NNS   JJ   ,  VBD    RB   VBN   IN  NNP          NNP     ,   DT      NN        NN      NN     ,   IN   NNP        ,    NNP    . \n",
      "   |       |       |     |   |     |       |      |     |          |     |     |     |   |    |   |   |        |    |    |   |     |     |    |    |            |      |   |       |         |       |      |   |     |         |     |     |  \n",
      "Trayvon Benjamin Martin was  an African American from Miami     Gardens  ,  Florida  ,  who   ,   at  17     years old   ,  was fatally shot  by George     Zimmerman  ,   a  neighborhood watch volunteer  ,   in Sanford      ,  Florida  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fourthSentParseTree[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or another sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ROOT                           \n",
      "                      |                              \n",
      "                      S                             \n",
      "       _______________|___________________________   \n",
      "      |                          VP               | \n",
      "      |                __________|___             |  \n",
      "      |               |              PP           | \n",
      "      |               |      ________|___         |  \n",
      "      NP              |     |            NP       | \n",
      "  ____|__________     |     |     _______|____    |  \n",
      " DT   JJ    JJ   NN  VBD    IN   DT      JJ   NN  . \n",
      " |    |     |    |    |     |    |       |    |   |  \n",
      "The quick brown fox jumped over the     lazy dog  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "list(parses[1])[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency parsing and graph representations\n",
    "\n",
    "Dependency parsing was developed to robustly capture linguistic dependencies from text. The complex tags associated with these parses are detailed [here]('http://universaldependencies.org/u/overview/syntax.html'). When parsing with the dependency parser, we will work directly from the untokenized text. Note that no *processing* takes place before parsing sentences--we do not remove so-called stop words or anything that plays a syntactic role in the sentence, although anaphora resolution and related normalization may be performed before or after parsing to enhance the value of information extraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I saw the elephant in my pajamas.',\n",
       " 'The quick brown fox jumped over the lazy dog.',\n",
       " 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.',\n",
       " 'Trayvon Benjamin Martin was an African American from Miami Gardens, Florida, who, at 17 years old, was fatally shot by George Zimmerman, a neighborhood watch volunteer, in Sanford, Florida.',\n",
       " 'Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function DependencyGraph.__init__.<locals>.<lambda> at 0x114d2cea0>,\n",
      "            {0: {'address': 0,\n",
      "                 'ctag': 'TOP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'root': [5]}),\n",
      "                 'feats': None,\n",
      "                 'head': None,\n",
      "                 'lemma': None,\n",
      "                 'rel': None,\n",
      "                 'tag': 'TOP',\n",
      "                 'word': None},\n",
      "             1: {'address': 1,\n",
      "                 'ctag': 'DT',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'det',\n",
      "                 'tag': 'DT',\n",
      "                 'word': 'The'},\n",
      "             2: {'address': 2,\n",
      "                 'ctag': 'JJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'amod',\n",
      "                 'tag': 'JJ',\n",
      "                 'word': 'quick'},\n",
      "             3: {'address': 3,\n",
      "                 'ctag': 'JJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'amod',\n",
      "                 'tag': 'JJ',\n",
      "                 'word': 'brown'},\n",
      "             4: {'address': 4,\n",
      "                 'ctag': 'NN',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'amod': [2, 3],\n",
      "                                      'det': [1]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 5,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'nsubj',\n",
      "                 'tag': 'NN',\n",
      "                 'word': 'fox'},\n",
      "             5: {'address': 5,\n",
      "                 'ctag': 'VBD',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'nmod': [9],\n",
      "                                      'nsubj': [4]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 0,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'root',\n",
      "                 'tag': 'VBD',\n",
      "                 'word': 'jumped'},\n",
      "             6: {'address': 6,\n",
      "                 'ctag': 'IN',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 9,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'case',\n",
      "                 'tag': 'IN',\n",
      "                 'word': 'over'},\n",
      "             7: {'address': 7,\n",
      "                 'ctag': 'DT',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 9,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'det',\n",
      "                 'tag': 'DT',\n",
      "                 'word': 'the'},\n",
      "             8: {'address': 8,\n",
      "                 'ctag': 'JJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 9,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'amod',\n",
      "                 'tag': 'JJ',\n",
      "                 'word': 'lazy'},\n",
      "             9: {'address': 9,\n",
      "                 'ctag': 'NN',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'amod': [8],\n",
      "                                      'case': [6],\n",
      "                                      'det': [7]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 5,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'nmod',\n",
      "                 'tag': 'NN',\n",
      "                 'word': 'dog'}})\n"
     ]
    }
   ],
   "source": [
    "depParses = list(stanford.depParser.raw_parse_sents(text)) #Converting the iterator to a list so we can call by index. They are still \n",
    "secondSentDepParseTree = list(depParses[1])[0] #iterators so be careful about re-running code, without re-running this block\n",
    "print(secondSentDepParseTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a graph and we can convert it to a dot file and use that to visulize it. Try traversing the tree and extracting elements that are nearby one another. We note that unless you have the graphviz successfully installed on your computer (which is not necessary to complete this homework), the following graphviz call will trigger an error. If you are interested in installing graphviz and working on a Mac, consider installing through [homebrew](https://brew.sh), a package manager (i.e., with the command \"brew install graphviz\", once brew is installed). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"467pt\" height=\"302pt\"\n",
       " viewBox=\"0.00 0.00 467.37 302.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 298)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-298 463.3657,-298 463.3657,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"193.7949\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"193.7949\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">5 (jumped)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;5 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M193.7949,-257.7616C193.7949,-246.3597 193.7949,-231.4342 193.7949,-218.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"197.295,-218.2121 193.7949,-208.2121 190.295,-218.2121 197.295,-218.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"205.0708\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"152.7949\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (fox)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;4 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>5&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M184.9506,-171.6975C182.2192,-166.0286 179.2067,-159.7594 176.457,-154 172.9867,-146.731 169.2503,-138.8578 165.7948,-131.5568\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"168.7462,-129.6107 161.3081,-122.0658 162.4177,-132.6024 168.7462,-129.6107\"/>\n",
       "<text text-anchor=\"middle\" x=\"191.9639\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"316.7949\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">9 (dog)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;9 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M219.5798,-171.9716C237.9481,-159.1286 262.8214,-141.7376 282.8098,-127.7619\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"284.8541,-130.6033 291.044,-122.0047 280.843,-124.8665 284.8541,-130.6033\"/>\n",
       "<text text-anchor=\"middle\" x=\"278.7397\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"28.7949\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 (The)</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"108.7949\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2 (quick)</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"195.7949\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3 (brown)</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>4&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M126.8005,-85.9716C108.2827,-73.1286 83.2073,-55.7376 63.0563,-41.7619\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"64.967,-38.8277 54.7552,-36.0047 60.9777,-44.5797 64.967,-38.8277\"/>\n",
       "<text text-anchor=\"middle\" x=\"107.3467\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>4&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M143.4637,-85.7616C137.4551,-74.0176 129.534,-58.5355 122.7802,-45.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"125.7834,-43.5204 118.1127,-36.2121 119.5517,-46.7088 125.7834,-43.5204\"/>\n",
       "<text text-anchor=\"middle\" x=\"149.3467\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">amod</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;3 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>4&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M161.9141,-85.7616C167.7861,-74.0176 175.5272,-58.5355 182.1275,-45.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"185.3472,-46.7216 186.6889,-36.2121 179.0862,-43.5911 185.3472,-46.7216\"/>\n",
       "<text text-anchor=\"middle\" x=\"193.3467\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">amod</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"279.7949\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">6 (over)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;6 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>9&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M308.9482,-85.7616C303.9446,-74.1316 297.3638,-58.8357 291.721,-45.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"294.7976,-44.0148 287.6304,-36.2121 288.3674,-46.7813 294.7976,-44.0148\"/>\n",
       "<text text-anchor=\"middle\" x=\"312.8398\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"354.7949\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">7 (the)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;7 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>9&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M324.8537,-85.7616C329.9926,-74.1316 336.7512,-58.8357 342.5466,-45.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"345.9074,-46.7736 346.7477,-36.2121 339.5046,-43.9444 345.9074,-46.7736\"/>\n",
       "<text text-anchor=\"middle\" x=\"347.3467\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"429.7949\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">8 (lazy)</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;8 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>9&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M340.4834,-85.9716C357.2078,-73.2433 379.8019,-56.0478 398.0799,-42.1371\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"400.2997,-44.8461 406.1376,-36.0047 396.0604,-39.2758 400.2997,-44.8461\"/>\n",
       "<text text-anchor=\"middle\" x=\"396.3467\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">amod</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x118b04358>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    secondSentGraph = graphviz.Source(secondSentDepParseTree.to_dot())\n",
    "except:\n",
    "    secondSentGraph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "secondSentGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or another sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"997pt\" height=\"560pt\"\n",
       " viewBox=\"0.00 0.00 996.96 560.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 556)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-556 992.9575,-556 992.9575,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"235.3833\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"235.3833\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">7 (American)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;7 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M235.3833,-515.7616C235.3833,-504.3597 235.3833,-489.4342 235.3833,-476.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"238.8834,-476.2121 235.3833,-466.2121 231.8834,-476.2121 238.8834,-476.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"246.6592\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"77.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3 (Martin)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;3 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>7&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M195.5259,-429.8504C183.9641,-424.3453 171.399,-418.1273 160.0454,-412 144.9107,-403.8322 128.6384,-394.1858 114.5678,-385.5556\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"116.0198,-382.3384 105.6735,-380.0569 112.3388,-388.2925 116.0198,-382.3384\"/>\n",
       "<text text-anchor=\"middle\" x=\"175.5522\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"161.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (was)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;4 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>7&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M219.6898,-429.7616C209.1921,-417.5615 195.2231,-401.3273 183.5899,-387.8076\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"186.2297,-385.5094 177.0542,-380.2121 180.9236,-390.0751 186.2297,-385.5094\"/>\n",
       "<text text-anchor=\"middle\" x=\"213.4902\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cop</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"235.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">5 (an)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;5 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>7&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M235.3833,-429.7616C235.3833,-418.3597 235.3833,-403.4342 235.3833,-390.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"238.8834,-390.2121 235.3833,-380.2121 231.8834,-390.2121 238.8834,-390.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"243.9351\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"319.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">6 (African)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;6 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>7&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M253.1975,-429.7616C265.2253,-417.4475 281.2673,-401.0235 294.5454,-387.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"297.1111,-389.8115 301.5947,-380.2121 292.1035,-384.9203 297.1111,-389.8115\"/>\n",
       "<text text-anchor=\"middle\" x=\"312.9351\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"421.3833\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">10 (Gardens)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;10 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M281.035,-436.267C301.523,-430.2297 325.6866,-422.0184 346.3833,-412 361.1468,-404.8536 376.3934,-395.1776 389.2526,-386.2414\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"391.6132,-388.8578 397.7385,-380.213 387.5592,-383.1511 391.6132,-388.8578\"/>\n",
       "<text text-anchor=\"middle\" x=\"386.3281\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"41.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 (Trayvon)</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"146.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2 (Benjamin)</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>3&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M69.7486,-343.7616C64.8803,-332.1316 58.4773,-316.8357 52.987,-303.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"56.097,-302.085 49.007,-294.2121 49.6399,-304.788 56.097,-302.085\"/>\n",
       "<text text-anchor=\"middle\" x=\"91.9351\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>3&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M105.7905,-343.9005C112.4839,-338.7035 119.1771,-332.628 124.3833,-326 129.5733,-319.3927 133.8059,-311.3704 137.1222,-303.7037\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"140.4118,-304.9025 140.8359,-294.3161 133.9027,-302.3275 140.4118,-304.9025\"/>\n",
       "<text text-anchor=\"middle\" x=\"162.9351\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"274.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">8 (from)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;8 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>10&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M376.5212,-344.6694C364.1537,-339.2688 350.9417,-332.9006 339.2935,-326 326.8103,-318.6047 313.97,-309.165 303.0389,-300.4839\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"305.0617,-297.6182 295.0895,-294.0398 300.6536,-303.0559 305.0617,-297.6182\"/>\n",
       "<text text-anchor=\"middle\" x=\"351.4282\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"360.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">9 (Miami)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;9 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>10&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M391.2568,-343.6347C384.9071,-338.5898 378.7774,-332.6501 374.2798,-326 369.9075,-319.5352 366.903,-311.7204 364.8413,-304.2096\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"368.2186,-303.2775 362.5744,-294.3112 361.3952,-304.8402 368.2186,-303.2775\"/>\n",
       "<text text-anchor=\"middle\" x=\"403.9351\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"456.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">12 (Florida)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;12 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>10&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M428.8059,-343.7616C433.539,-332.1316 439.7641,-316.8357 445.1019,-303.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"448.4436,-304.7938 448.9714,-294.2121 441.96,-302.1551 448.4436,-304.7938\"/>\n",
       "<text text-anchor=\"middle\" x=\"457.7144\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">appos</text>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>23</title>\n",
       "<text text-anchor=\"middle\" x=\"549.3833\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">23 (shot)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;23 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>10&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M450.562,-343.9652C459.5457,-338.3073 469.4264,-331.9769 478.3833,-326 490.7955,-317.7174 504.223,-308.3654 516.0254,-300.0089\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"518.1726,-302.7768 524.2944,-294.1295 514.1163,-297.0718 518.1726,-302.7768\"/>\n",
       "<text text-anchor=\"middle\" x=\"520.9214\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">acl:relcl</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>14</title>\n",
       "<text text-anchor=\"middle\" x=\"341.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">14 (who)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;14 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>23&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M516.1602,-260.9769C513.2216,-259.8878 510.2679,-258.8756 507.3833,-258 465.6572,-245.3346 451.0323,-257.1374 410.9351,-240 396.1578,-233.6843 381.463,-223.7913 369.4142,-214.4665\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"371.5209,-211.6696 361.5269,-208.1526 367.1463,-217.1344 371.5209,-211.6696\"/>\n",
       "<text text-anchor=\"middle\" x=\"438.6074\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubjpass</text>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>19</title>\n",
       "<text text-anchor=\"middle\" x=\"423.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">19 (old)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;19 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>23&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M516.1838,-257.9822C506.6151,-252.48 496.2767,-246.2305 487.0659,-240 475.5821,-232.2319 463.494,-223.0275 452.972,-214.6466\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"454.892,-211.6986 444.9108,-208.1451 450.4974,-217.1473 454.892,-211.6986\"/>\n",
       "<text text-anchor=\"middle\" x=\"502.542\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advcl</text>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>21</title>\n",
       "<text text-anchor=\"middle\" x=\"504.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">21 (was)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;21 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>23&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M538.2032,-257.8722C534.8316,-252.2105 531.1955,-245.9014 528.0591,-240 524.2785,-232.8864 520.4337,-225.0669 516.9726,-217.7676\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"519.9328,-215.8341 512.5329,-208.2517 513.5892,-218.7937 519.9328,-215.8341\"/>\n",
       "<text text-anchor=\"middle\" x=\"550.5454\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">auxpass</text>\n",
       "</g>\n",
       "<!-- 22 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>22</title>\n",
       "<text text-anchor=\"middle\" x=\"594.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">22 (fatally)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;22 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>23&#45;&gt;22</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M561.88,-257.643C565.455,-252.0813 569.2329,-245.8848 572.3833,-240 576.1375,-232.9872 579.7933,-225.2024 583.0121,-217.9043\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"586.3752,-218.9423 587.0989,-208.3722 579.9415,-216.1839 586.3752,-218.9423\"/>\n",
       "<text text-anchor=\"middle\" x=\"601.9351\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>26</title>\n",
       "<text text-anchor=\"middle\" x=\"707.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">26 (Zimmerman)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;26 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>23&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M582.5441,-262.0972C596.8621,-255.7864 613.6686,-247.961 628.3833,-240 643.1389,-232.0169 658.8574,-222.3295 672.3559,-213.6153\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"674.4115,-216.4533 680.8734,-208.0572 670.586,-210.5911 674.4115,-216.4533\"/>\n",
       "<text text-anchor=\"middle\" x=\"667.3281\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 36 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>36</title>\n",
       "<text text-anchor=\"middle\" x=\"856.3833\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">36 (Florida)</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;36 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>23&#45;&gt;36</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M582.6464,-267.6451C610.6488,-260.5344 651.722,-249.924 687.3833,-240 726.7862,-229.0348 771.1175,-215.8883 804.4873,-205.8282\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"805.9136,-209.0536 814.474,-202.811 803.889,-202.3527 805.9136,-209.0536\"/>\n",
       "<text text-anchor=\"middle\" x=\"750.3281\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>16</title>\n",
       "<text text-anchor=\"middle\" x=\"391.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">16 (at)</text>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>17</title>\n",
       "<text text-anchor=\"middle\" x=\"472.3833\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">17 (17)</text>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>18</title>\n",
       "<text text-anchor=\"middle\" x=\"472.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">18 (years)</text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;17 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>18&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M472.3833,-85.7616C472.3833,-74.3597 472.3833,-59.4342 472.3833,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"475.8834,-46.2121 472.3833,-36.2121 468.8834,-46.2121 475.8834,-46.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"497.2729\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nummod</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;16 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>19&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M416.5969,-171.7616C412.2695,-160.1316 406.578,-144.8357 401.6977,-131.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"404.9276,-130.3637 398.1599,-122.2121 398.367,-132.8049 404.9276,-130.3637\"/>\n",
       "<text text-anchor=\"middle\" x=\"421.4282\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;18 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>19&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M433.7749,-171.7616C440.5313,-159.9036 449.459,-144.2345 457.0275,-130.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"460.0971,-132.6334 462.0066,-122.2121 454.0151,-129.1681 460.0971,-132.6334\"/>\n",
       "<text text-anchor=\"middle\" x=\"488.7178\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod:npmod</text>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>24</title>\n",
       "<text text-anchor=\"middle\" x=\"567.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">24 (by)</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;24 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>26&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M668.1869,-171.9496C657.328,-166.5419 645.6763,-160.3501 635.2935,-154 622.6297,-146.2549 609.3446,-136.8017 597.9168,-128.2084\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"600.0333,-125.4208 589.9593,-122.1398 595.7884,-130.9869 600.0333,-125.4208\"/>\n",
       "<text text-anchor=\"middle\" x=\"647.4282\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>25</title>\n",
       "<text text-anchor=\"middle\" x=\"655.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">25 (George)</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;25 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>26&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M684.2392,-171.951C678.6851,-166.6739 673.2261,-160.5473 669.2798,-154 665.2509,-147.3157 662.3575,-139.4301 660.2923,-131.9215\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"663.6687,-130.9902 657.9608,-122.0643 656.8567,-132.6015 663.6687,-130.9902\"/>\n",
       "<text text-anchor=\"middle\" x=\"698.9351\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 31 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>31</title>\n",
       "<text text-anchor=\"middle\" x=\"763.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">31 (volunteer)</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;31 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>26&#45;&gt;31</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M719.2595,-171.7616C726.981,-159.9036 737.1841,-144.2345 745.8338,-130.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"749.0005,-132.502 751.5243,-122.2121 743.1345,-128.6822 749.0005,-132.502\"/>\n",
       "<text text-anchor=\"middle\" x=\"755.7144\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">appos</text>\n",
       "</g>\n",
       "<!-- 33 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>33</title>\n",
       "<text text-anchor=\"middle\" x=\"856.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">33 (in)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;33 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>36&#45;&gt;33</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M856.3833,-171.7616C856.3833,-160.3597 856.3833,-145.4342 856.3833,-132.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"859.8834,-132.2121 856.3833,-122.2121 852.8834,-132.2121 859.8834,-132.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"868.4282\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 34 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>34</title>\n",
       "<text text-anchor=\"middle\" x=\"945.3833\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">34 (Sanford)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;34 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>36&#45;&gt;34</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M875.2579,-171.7616C888.0016,-159.4475 904.9985,-143.0235 919.067,-129.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"921.7767,-131.6779 926.5359,-122.2121 916.9125,-126.644 921.7767,-131.6779\"/>\n",
       "<text text-anchor=\"middle\" x=\"934.9351\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>28</title>\n",
       "<text text-anchor=\"middle\" x=\"658.3833\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">28 (a)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;28 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>31&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M741.3719,-85.9716C725.9715,-73.358 705.2148,-56.3573 688.313,-42.5139\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"690.3198,-39.6334 680.3658,-36.0047 685.8843,-45.0488 690.3198,-39.6334\"/>\n",
       "<text text-anchor=\"middle\" x=\"725.9351\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>29</title>\n",
       "<text text-anchor=\"middle\" x=\"763.3833\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">29 (neighborhood)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;29 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>31&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M763.3833,-85.7616C763.3833,-74.3597 763.3833,-59.4342 763.3833,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"766.8834,-46.2121 763.3833,-36.2121 759.8834,-46.2121 766.8834,-46.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"792.9351\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 30 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>30</title>\n",
       "<text text-anchor=\"middle\" x=\"879.3833\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">30 (watch)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;30 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>31&#45;&gt;30</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M798.3154,-85.9914C807.713,-80.629 817.6789,-74.4499 826.3833,-68 836.2881,-60.6606 846.3323,-51.6634 854.9744,-43.3341\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"857.5027,-45.7565 862.1694,-36.2448 852.5896,-40.7703 857.5027,-45.7565\"/>\n",
       "<text text-anchor=\"middle\" x=\"873.9351\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x11804b2e8>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    graph = graphviz.Source(list(depParses[3])[0].to_dot())\n",
    "except IndexError:\n",
    "    print(\"You likely have to rerun the depParses\")\n",
    "    raise\n",
    "except:\n",
    "    graph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do a dependency parse on the reddit sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topPostDepParse = list(stanford.depParser.parse_sents(redditTopScores['sentences'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes a few seconds, but now lets look at the parse tree from one of the processed sentences.\n",
    "\n",
    "The sentence is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So anyway , I get a call from an older gentleman who 's quite bitter and mean right off the bat ( does n't like that I asked for his address / telephone number to verify the account , hates that he has to speak with a machine before reaching an agent , etc . ) .\n"
     ]
    }
   ],
   "source": [
    "targetSentence = 7\n",
    "print(' '.join(redditTopScores['sentences'][0][targetSentence]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which leads to a very rich dependancy tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"1113pt\" height=\"818pt\"\n",
       " viewBox=\"0.00 0.00 1112.57 818.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 814)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-814 1108.5708,-814 1108.5708,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"197\" y=\"-787.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"197\" y=\"-701.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">5 (get)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;5 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M197,-773.7616C197,-762.3597 197,-747.4342 197,-734.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"200.5001,-734.2121 197,-724.2121 193.5001,-734.2121 200.5001,-734.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"208.2759\" y=\"-744.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-615.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 (So)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>5&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M169.8559,-700.0143C147.0611,-694.3052 114.1086,-684.4249 87.8965,-670 75.603,-663.2347 63.384,-653.8632 53.1537,-645.0603\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"55.2127,-642.2079 45.4121,-638.1831 50.5637,-647.4412 55.2127,-642.2079\"/>\n",
       "<text text-anchor=\"middle\" x=\"110.5518\" y=\"-658.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"112\" y=\"-615.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2 (anyway)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>5&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M169.7699,-688.3776C162.2887,-682.9299 154.4412,-676.6103 147.8965,-670 140.9396,-662.9735 134.3222,-654.4941 128.7076,-646.5301\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"131.5534,-644.4907 123.0402,-638.1838 125.7623,-648.423 131.5534,-644.4907\"/>\n",
       "<text text-anchor=\"middle\" x=\"170.5518\" y=\"-658.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"197\" y=\"-615.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (I)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>5&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M197,-687.7616C197,-676.3597 197,-661.4342 197,-648.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"200.5001,-648.2121 197,-638.2121 193.5001,-648.2121 200.5001,-648.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"212.1689\" y=\"-658.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"270\" y=\"-615.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">7 (call)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;7 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>5&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M214.6382,-687.9935C220.0064,-682.3367 225.8468,-675.9998 231,-670 237.4265,-662.5177 244.1316,-654.1485 250.1331,-646.4296\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"253.0006,-648.4423 256.3228,-638.3818 247.4519,-644.1747 253.0006,-648.4423\"/>\n",
       "<text text-anchor=\"middle\" x=\"255.4448\" y=\"-658.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dobj</text>\n",
       "</g>\n",
       "<!-- 34 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>34</title>\n",
       "<text text-anchor=\"middle\" x=\"643\" y=\"-615.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">34 (number)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;34 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;34</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M224.2279,-700.7498C296.5806,-686.7984 494.6077,-648.6138 590.1104,-630.1985\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"590.9741,-633.5965 600.1305,-628.2663 589.6487,-626.7231 590.9741,-633.5965\"/>\n",
       "<text text-anchor=\"middle\" x=\"457.1069\" y=\"-658.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dep</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"175\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">6 (a)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;6 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>7&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M249.8529,-601.7616C236.1242,-589.3335 217.771,-572.719 202.6742,-559.0524\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"204.8805,-556.3286 195.118,-552.2121 200.1826,-561.518 204.8805,-556.3286\"/>\n",
       "<text text-anchor=\"middle\" x=\"237.5518\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>11</title>\n",
       "<text text-anchor=\"middle\" x=\"270\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">11 (gentleman)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;11 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M270,-601.7616C270,-590.3597 270,-575.4342 270,-562.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"273.5001,-562.2121 270,-552.2121 266.5001,-562.2121 273.5001,-562.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"285.9448\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>25</title>\n",
       "<text text-anchor=\"middle\" x=\"569\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">25 (like)</text>\n",
       "</g>\n",
       "<!-- 34&#45;&gt;25 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>34&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M627.3065,-601.7616C616.8088,-589.5615 602.8398,-573.3273 591.2066,-559.8076\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"593.8464,-557.5094 584.6709,-552.2121 588.5403,-562.0751 593.8464,-557.5094\"/>\n",
       "<text text-anchor=\"middle\" x=\"621.1069\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dep</text>\n",
       "</g>\n",
       "<!-- 33 -->\n",
       "<g id=\"node32\" class=\"node\">\n",
       "<title>33</title>\n",
       "<text text-anchor=\"middle\" x=\"668\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">33 (telephone)</text>\n",
       "</g>\n",
       "<!-- 34&#45;&gt;33 -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>34&#45;&gt;33</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M648.3019,-601.7616C651.6495,-590.2456 656.0421,-575.1353 659.8295,-562.1064\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"663.2752,-562.7916 662.7058,-552.2121 656.5534,-560.8376 663.2752,-562.7916\"/>\n",
       "<text text-anchor=\"middle\" x=\"686.5518\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compound</text>\n",
       "</g>\n",
       "<!-- 36 -->\n",
       "<g id=\"node33\" class=\"node\">\n",
       "<title>36</title>\n",
       "<text text-anchor=\"middle\" x=\"780\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">36 (verify)</text>\n",
       "</g>\n",
       "<!-- 34&#45;&gt;36 -->\n",
       "<g id=\"edge32\" class=\"edge\">\n",
       "<title>34&#45;&gt;36</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M685.5984,-602.4626C697.0839,-597.1021 709.2884,-590.8061 720,-584 731.5591,-576.6554 743.3366,-567.3235 753.361,-558.7155\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"755.7821,-561.2474 760.9868,-552.019 751.1633,-555.9874 755.7821,-561.2474\"/>\n",
       "<text text-anchor=\"middle\" x=\"748.1587\" y=\"-572.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">acl</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"112\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">8 (from)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;8 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>11&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M231.7907,-515.847C220.4724,-510.291 208.113,-504.0465 196.9102,-498 181.4511,-489.6562 164.732,-479.9612 150.251,-471.3415\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"151.9329,-468.2691 141.5551,-466.1352 148.3371,-474.275 151.9329,-468.2691\"/>\n",
       "<text text-anchor=\"middle\" x=\"209.0449\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"189\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">9 (an)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;9 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>11&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M252.822,-515.7616C241.3312,-503.5615 226.0409,-487.3273 213.3072,-473.8076\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"215.5574,-471.0919 206.1533,-466.2121 210.4617,-475.8913 215.5574,-471.0919\"/>\n",
       "<text text-anchor=\"middle\" x=\"243.5518\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"270\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">10 (older)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;10 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>11&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M270,-515.7616C270,-504.3597 270,-489.4342 270,-476.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"273.5001,-476.2121 270,-466.2121 266.5001,-476.2121 273.5001,-476.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"285.5518\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">amod</text>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>15</title>\n",
       "<text text-anchor=\"middle\" x=\"360\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">15 (bitter)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;15 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>11&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M289.0867,-515.7616C301.9735,-503.4475 319.1614,-487.0235 333.388,-473.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"336.1289,-475.6512 340.9408,-466.2121 331.2929,-470.5902 336.1289,-475.6512\"/>\n",
       "<text text-anchor=\"middle\" x=\"343.5381\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">acl:relcl</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"160\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">12 (who)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;12 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>15&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M323.7668,-433.1476C320.814,-432.047 317.8649,-430.9839 315,-430 287.7259,-420.6334 279.3566,-422.9093 252.6621,-412 234.2444,-404.4732 214.7375,-394.3082 198.3969,-385.109\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"200.0902,-382.0455 189.6716,-380.1206 196.6158,-388.1224 200.0902,-382.0455\"/>\n",
       "<text text-anchor=\"middle\" x=\"268.1689\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>13</title>\n",
       "<text text-anchor=\"middle\" x=\"238\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">13 (&#39;s)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;13 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>15&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M332.2421,-429.9468C323.6973,-424.2881 314.3008,-417.9619 305.7861,-412 294.0448,-403.7789 281.3575,-394.5234 270.1697,-386.2309\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"272.0587,-383.2738 261.9472,-380.1103 267.8789,-388.8889 272.0587,-383.2738\"/>\n",
       "<text text-anchor=\"middle\" x=\"316.1069\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cop</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>14</title>\n",
       "<text text-anchor=\"middle\" x=\"318\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">14 (quite)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;14 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>15&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M346.5824,-429.8451C342.8269,-424.2891 338.9433,-418.045 335.8965,-412 332.3589,-404.9812 329.1928,-397.0923 326.543,-389.6912\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"329.7918,-388.373 323.2634,-380.0286 323.1632,-390.6229 329.7918,-388.373\"/>\n",
       "<text text-anchor=\"middle\" x=\"358.5518\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>16</title>\n",
       "<text text-anchor=\"middle\" x=\"403\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">16 (and)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;16 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>15&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M371.0917,-429.9088C374.4154,-424.2486 377.9776,-417.9311 381,-412 384.6116,-404.9127 388.2182,-397.1023 391.4354,-389.8033\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"394.7951,-390.8514 395.5451,-380.2831 388.3684,-388.0771 394.7951,-390.8514\"/>\n",
       "<text text-anchor=\"middle\" x=\"394.2139\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cc</text>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>17</title>\n",
       "<text text-anchor=\"middle\" x=\"489\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">17 (mean)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;17 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>15&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M387.0426,-429.9716C406.393,-417.0713 432.6266,-399.5822 453.6376,-385.575\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"455.6139,-388.4639 461.993,-380.0047 451.731,-382.6395 455.6139,-388.4639\"/>\n",
       "<text text-anchor=\"middle\" x=\"445.0518\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">conj</text>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>18</title>\n",
       "<text text-anchor=\"middle\" x=\"451\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">18 (right)</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;18 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>17&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M470.6407,-343.8974C466.1437,-338.5346 461.7952,-332.3788 458.8965,-326 455.8322,-319.2569 453.9297,-311.5343 452.7561,-304.2081\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"456.2273,-303.7587 451.5373,-294.2585 449.2792,-304.6099 456.2273,-303.7587\"/>\n",
       "<text text-anchor=\"middle\" x=\"481.5518\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>21</title>\n",
       "<text text-anchor=\"middle\" x=\"534\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">21 (bat)</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;21 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>17&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M498.5433,-343.7616C504.6885,-332.0176 512.7896,-316.5355 519.6969,-303.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"522.9353,-304.6951 524.4704,-294.2121 516.733,-301.4497 522.9353,-304.6951\"/>\n",
       "<text text-anchor=\"middle\" x=\"530.9448\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>19</title>\n",
       "<text text-anchor=\"middle\" x=\"478\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">19 (off)</text>\n",
       "</g>\n",
       "<!-- 21&#45;&gt;19 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>21&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M522.1238,-257.7616C514.4023,-245.9036 504.1992,-230.2345 495.5495,-216.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"498.2488,-214.6822 489.859,-208.2121 492.3828,-218.502 498.2488,-214.6822\"/>\n",
       "<text text-anchor=\"middle\" x=\"522.0449\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 20 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>20</title>\n",
       "<text text-anchor=\"middle\" x=\"555\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">20 (the)</text>\n",
       "</g>\n",
       "<!-- 21&#45;&gt;20 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>21&#45;&gt;20</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M538.4536,-257.7616C541.2656,-246.2456 544.9553,-231.1353 548.1368,-218.1064\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"551.5807,-218.757 550.5529,-208.2121 544.7805,-217.0964 551.5807,-218.757\"/>\n",
       "<text text-anchor=\"middle\" x=\"554.5518\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>23</title>\n",
       "<text text-anchor=\"middle\" x=\"489\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">23 (does)</text>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>24</title>\n",
       "<text text-anchor=\"middle\" x=\"569\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">24 (n&#39;t)</text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;23 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>25&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M552.0341,-515.7616C540.6851,-503.5615 525.5836,-487.3273 513.0071,-473.8076\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"515.3152,-471.1501 505.9415,-466.2121 510.1899,-475.9179 515.3152,-471.1501\"/>\n",
       "<text text-anchor=\"middle\" x=\"544.1069\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">aux</text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;24 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>25&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M569,-515.7616C569,-504.3597 569,-489.4342 569,-476.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"572.5001,-476.2121 569,-466.2121 565.5001,-476.2121 572.5001,-476.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"579.1069\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">neg</text>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>28</title>\n",
       "<text text-anchor=\"middle\" x=\"652\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">28 (asked)</text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;28 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>25&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M586.6022,-515.7616C598.4867,-503.4475 614.3377,-487.0235 627.4578,-473.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"629.9971,-475.8382 634.4232,-466.2121 624.9603,-470.977 629.9971,-475.8382\"/>\n",
       "<text text-anchor=\"middle\" x=\"634.6587\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ccomp</text>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>26</title>\n",
       "<text text-anchor=\"middle\" x=\"575\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">26 (that)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;26 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>28&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M635.2812,-429.7312C630.1192,-424.0637 624.4271,-417.7868 619.2344,-412 612.1517,-404.1071 604.4811,-395.4688 597.5476,-387.6264\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"600.1509,-385.2867 590.9089,-380.1063 594.9032,-389.9194 600.1509,-385.2867\"/>\n",
       "<text text-anchor=\"middle\" x=\"634.3828\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 27 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>27</title>\n",
       "<text text-anchor=\"middle\" x=\"652\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">27 (I)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;27 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>28&#45;&gt;27</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M652,-429.7616C652,-418.3597 652,-403.4342 652,-390.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"655.5001,-390.2121 652,-380.2121 648.5001,-390.2121 655.5001,-390.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"667.1689\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 31 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>31</title>\n",
       "<text text-anchor=\"middle\" x=\"739\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">31 (address)</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;31 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>28&#45;&gt;31</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M670.4505,-429.7616C682.9077,-417.4475 699.5227,-401.0235 713.275,-387.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"715.9248,-389.7314 720.5761,-380.2121 711.0037,-384.7531 715.9248,-389.7314\"/>\n",
       "<text text-anchor=\"middle\" x=\"716.9448\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>29</title>\n",
       "<text text-anchor=\"middle\" x=\"662\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">29 (for)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;29 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>31&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M722.6703,-343.7616C711.747,-331.5615 697.2117,-315.3273 685.1068,-301.8076\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"687.5843,-299.3276 678.3062,-294.2121 682.3691,-303.9969 687.5843,-299.3276\"/>\n",
       "<text text-anchor=\"middle\" x=\"718.0449\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 30 -->\n",
       "<g id=\"node31\" class=\"node\">\n",
       "<title>30</title>\n",
       "<text text-anchor=\"middle\" x=\"739\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">30 (his)</text>\n",
       "</g>\n",
       "<!-- 31&#45;&gt;30 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>31&#45;&gt;30</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M739,-343.7616C739,-332.3597 739,-317.4342 739,-304.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"742.5001,-304.2121 739,-294.2121 735.5001,-304.2121 742.5001,-304.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"769.3379\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod:poss</text>\n",
       "</g>\n",
       "<!-- 35 -->\n",
       "<g id=\"node34\" class=\"node\">\n",
       "<title>35</title>\n",
       "<text text-anchor=\"middle\" x=\"780\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">35 (to)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;35 -->\n",
       "<g id=\"edge33\" class=\"edge\">\n",
       "<title>36&#45;&gt;35</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M780,-515.7616C780,-504.3597 780,-489.4342 780,-476.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"783.5001,-476.2121 780,-466.2121 776.5001,-476.2121 783.5001,-476.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"794.3828\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 38 -->\n",
       "<g id=\"node35\" class=\"node\">\n",
       "<title>38</title>\n",
       "<text text-anchor=\"middle\" x=\"883\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">38 (account)</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;38 -->\n",
       "<g id=\"edge34\" class=\"edge\">\n",
       "<title>36&#45;&gt;38</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M801.8437,-515.7616C816.8651,-503.2195 836.9927,-486.4138 853.445,-472.677\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"855.7549,-475.3079 861.1878,-466.2121 851.2685,-469.9347 855.7549,-475.3079\"/>\n",
       "<text text-anchor=\"middle\" x=\"850.4448\" y=\"-486.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dobj</text>\n",
       "</g>\n",
       "<!-- 37 -->\n",
       "<g id=\"node36\" class=\"node\">\n",
       "<title>37</title>\n",
       "<text text-anchor=\"middle\" x=\"829\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">37 (the)</text>\n",
       "</g>\n",
       "<!-- 38&#45;&gt;37 -->\n",
       "<g id=\"edge35\" class=\"edge\">\n",
       "<title>38&#45;&gt;37</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M871.548,-429.7616C864.1023,-417.9036 854.2635,-402.2345 845.9227,-388.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"848.7174,-386.8198 840.4355,-380.2121 842.7891,-390.5422 848.7174,-386.8198\"/>\n",
       "<text text-anchor=\"middle\" x=\"868.5518\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 40 -->\n",
       "<g id=\"node37\" class=\"node\">\n",
       "<title>40</title>\n",
       "<text text-anchor=\"middle\" x=\"913\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">40 (hates)</text>\n",
       "</g>\n",
       "<!-- 38&#45;&gt;40 -->\n",
       "<g id=\"edge36\" class=\"edge\">\n",
       "<title>38&#45;&gt;40</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M889.3622,-429.7616C893.4192,-418.1316 898.755,-402.8357 903.3302,-389.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"906.6579,-390.8069 906.6469,-380.2121 900.0485,-388.5013 906.6579,-390.8069\"/>\n",
       "<text text-anchor=\"middle\" x=\"912.0518\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">conj</text>\n",
       "</g>\n",
       "<!-- 54 -->\n",
       "<g id=\"node38\" class=\"node\">\n",
       "<title>54</title>\n",
       "<text text-anchor=\"middle\" x=\"996\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">54 (etc)</text>\n",
       "</g>\n",
       "<!-- 38&#45;&gt;54 -->\n",
       "<g id=\"edge37\" class=\"edge\">\n",
       "<title>38&#45;&gt;54</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M906.6885,-429.9716C923.4129,-417.2433 946.007,-400.0478 964.285,-386.1371\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"966.5048,-388.8461 972.3427,-380.0047 962.2654,-383.2758 966.5048,-388.8461\"/>\n",
       "<text text-anchor=\"middle\" x=\"959.0518\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">conj</text>\n",
       "</g>\n",
       "<!-- 43 -->\n",
       "<g id=\"node39\" class=\"node\">\n",
       "<title>43</title>\n",
       "<text text-anchor=\"middle\" x=\"817\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">43 (has)</text>\n",
       "</g>\n",
       "<!-- 40&#45;&gt;43 -->\n",
       "<g id=\"edge38\" class=\"edge\">\n",
       "<title>40&#45;&gt;43</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M892.6409,-343.7616C878.7676,-331.3335 860.2213,-314.719 844.9655,-301.0524\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"847.1135,-298.2777 837.3298,-294.2121 842.4428,-303.4915 847.1135,-298.2777\"/>\n",
       "<text text-anchor=\"middle\" x=\"889.6587\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ccomp</text>\n",
       "</g>\n",
       "<!-- 50 -->\n",
       "<g id=\"node40\" class=\"node\">\n",
       "<title>50</title>\n",
       "<text text-anchor=\"middle\" x=\"974\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">50 (reaching)</text>\n",
       "</g>\n",
       "<!-- 40&#45;&gt;50 -->\n",
       "<g id=\"edge39\" class=\"edge\">\n",
       "<title>40&#45;&gt;50</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M925.9365,-343.7616C934.4283,-331.7896 945.6756,-315.9328 955.1549,-302.5685\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"958.1514,-304.3935 961.0821,-294.2121 952.4419,-300.3437 958.1514,-304.3935\"/>\n",
       "<text text-anchor=\"middle\" x=\"963.1587\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advcl</text>\n",
       "</g>\n",
       "<!-- 41 -->\n",
       "<g id=\"node41\" class=\"node\">\n",
       "<title>41</title>\n",
       "<text text-anchor=\"middle\" x=\"718\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">41 (that)</text>\n",
       "</g>\n",
       "<!-- 43&#45;&gt;41 -->\n",
       "<g id=\"edge40\" class=\"edge\">\n",
       "<title>43&#45;&gt;41</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M794.5653,-257.8876C787.6662,-252.2266 780.0866,-245.9139 773.2344,-240 763.9189,-231.9602 753.8992,-222.991 744.9852,-214.8934\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"747.228,-212.2018 737.4818,-208.047 742.5098,-217.3728 747.228,-212.2018\"/>\n",
       "<text text-anchor=\"middle\" x=\"788.3828\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 42 -->\n",
       "<g id=\"node42\" class=\"node\">\n",
       "<title>42</title>\n",
       "<text text-anchor=\"middle\" x=\"796\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">42 (he)</text>\n",
       "</g>\n",
       "<!-- 43&#45;&gt;42 -->\n",
       "<g id=\"edge41\" class=\"edge\">\n",
       "<title>43&#45;&gt;42</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M812.5464,-257.7616C809.7344,-246.2456 806.0447,-231.1353 802.8632,-218.1064\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"806.2195,-217.0964 800.4471,-208.2121 799.4193,-218.757 806.2195,-217.0964\"/>\n",
       "<text text-anchor=\"middle\" x=\"823.1689\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 45 -->\n",
       "<g id=\"node43\" class=\"node\">\n",
       "<title>45</title>\n",
       "<text text-anchor=\"middle\" x=\"879\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">45 (speak)</text>\n",
       "</g>\n",
       "<!-- 43&#45;&gt;45 -->\n",
       "<g id=\"edge42\" class=\"edge\">\n",
       "<title>43&#45;&gt;45</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M830.1486,-257.7616C838.7796,-245.7896 850.2112,-229.9328 859.8459,-216.5685\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"862.8614,-218.3707 865.8703,-208.2121 857.1832,-214.2771 862.8614,-218.3707\"/>\n",
       "<text text-anchor=\"middle\" x=\"871.0518\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">xcomp</text>\n",
       "</g>\n",
       "<!-- 49 -->\n",
       "<g id=\"node48\" class=\"node\">\n",
       "<title>49</title>\n",
       "<text text-anchor=\"middle\" x=\"974\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">49 (before)</text>\n",
       "</g>\n",
       "<!-- 50&#45;&gt;49 -->\n",
       "<g id=\"edge47\" class=\"edge\">\n",
       "<title>50&#45;&gt;49</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M974,-257.7616C974,-246.3597 974,-231.4342 974,-218.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"977.5001,-218.2121 974,-208.2121 970.5001,-218.2121 977.5001,-218.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"988.3828\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 52 -->\n",
       "<g id=\"node49\" class=\"node\">\n",
       "<title>52</title>\n",
       "<text text-anchor=\"middle\" x=\"1068\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">52 (agent)</text>\n",
       "</g>\n",
       "<!-- 50&#45;&gt;52 -->\n",
       "<g id=\"edge48\" class=\"edge\">\n",
       "<title>50&#45;&gt;52</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M993.935,-257.7616C1007.5192,-245.3335 1025.6792,-228.719 1040.6172,-215.0524\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1043.0782,-217.5446 1048.0938,-208.2121 1038.3531,-212.3799 1043.0782,-217.5446\"/>\n",
       "<text text-anchor=\"middle\" x=\"1039.4448\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dobj</text>\n",
       "</g>\n",
       "<!-- 44 -->\n",
       "<g id=\"node44\" class=\"node\">\n",
       "<title>44</title>\n",
       "<text text-anchor=\"middle\" x=\"844\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">44 (to)</text>\n",
       "</g>\n",
       "<!-- 45&#45;&gt;44 -->\n",
       "<g id=\"edge43\" class=\"edge\">\n",
       "<title>45&#45;&gt;44</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M871.4717,-171.6927C869.1458,-166.0237 866.5793,-159.7555 864.2344,-154 861.2709,-146.7261 858.0772,-138.8513 855.1223,-131.5502\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"858.2784,-130.0188 851.2849,-122.06 851.7888,-132.6429 858.2784,-130.0188\"/>\n",
       "<text text-anchor=\"middle\" x=\"879.3828\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 48 -->\n",
       "<g id=\"node45\" class=\"node\">\n",
       "<title>48</title>\n",
       "<text text-anchor=\"middle\" x=\"934\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">48 (machine)</text>\n",
       "</g>\n",
       "<!-- 45&#45;&gt;48 -->\n",
       "<g id=\"edge44\" class=\"edge\">\n",
       "<title>45&#45;&gt;48</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M890.6641,-171.7616C898.2477,-159.9036 908.2686,-144.2345 916.7639,-130.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"919.9135,-132.5223 922.3527,-122.2121 914.0163,-128.7509 919.9135,-132.5223\"/>\n",
       "<text text-anchor=\"middle\" x=\"925.9448\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 46 -->\n",
       "<g id=\"node46\" class=\"node\">\n",
       "<title>46</title>\n",
       "<text text-anchor=\"middle\" x=\"894\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">46 (with)</text>\n",
       "</g>\n",
       "<!-- 48&#45;&gt;46 -->\n",
       "<g id=\"edge45\" class=\"edge\">\n",
       "<title>48&#45;&gt;46</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M925.517,-85.7616C920.0547,-74.0176 912.8537,-58.5355 906.7139,-45.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"909.8616,-43.8032 902.4707,-36.2121 903.5146,-46.7554 909.8616,-43.8032\"/>\n",
       "<text text-anchor=\"middle\" x=\"929.0449\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 47 -->\n",
       "<g id=\"node47\" class=\"node\">\n",
       "<title>47</title>\n",
       "<text text-anchor=\"middle\" x=\"973\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">47 (a)</text>\n",
       "</g>\n",
       "<!-- 48&#45;&gt;47 -->\n",
       "<g id=\"edge46\" class=\"edge\">\n",
       "<title>48&#45;&gt;47</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M942.2709,-85.7616C947.5967,-74.0176 954.6176,-58.5355 960.604,-45.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"963.7985,-46.7649 964.741,-36.2121 957.4234,-43.8739 963.7985,-46.7649\"/>\n",
       "<text text-anchor=\"middle\" x=\"964.5518\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 51 -->\n",
       "<g id=\"node50\" class=\"node\">\n",
       "<title>51</title>\n",
       "<text text-anchor=\"middle\" x=\"1068\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">51 (an)</text>\n",
       "</g>\n",
       "<!-- 52&#45;&gt;51 -->\n",
       "<g id=\"edge49\" class=\"edge\">\n",
       "<title>52&#45;&gt;51</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1068,-171.7616C1068,-160.3597 1068,-145.4342 1068,-132.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1071.5001,-132.2121 1068,-122.2121 1064.5001,-132.2121 1071.5001,-132.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"1076.5518\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x119e92b38>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    graph = graphviz.Source(list(topPostDepParse[targetSentence])[0].to_dot())\n",
    "except IndexError:\n",
    "    print(\"You likely have to rerun the depParses\")\n",
    "    raise\n",
    "except:\n",
    "    graph = None\n",
    "    print(\"There was a problem with graphviz, likely your missing the program, https://www.graphviz.org/download/\")\n",
    "graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 3*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, parse a (modest) subset of your corpus of interest. How deep are the phrase structure and dependency parse trees nested? How does parse depth relate to perceived sentence complexity? What are five things you can extract from these parses for subsequent analysis? (e.g., nouns collocated in a noun phrase; adjectives that modify a noun; etc.) Capture these sets of things for a focal set of words (e.g., \"Bush\", \"Obama\", \"Trump\"). What do they reveal about the roles that these entities are perceived to play in the social world inscribed by your texts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "depParses = list(stanford.depParser.raw_parse_sents(text)) #Converting the iterator to a list so we can call by index. They are still "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Young people in Singapore aged 18 to 29 are at a higher risk of having mental problems than older people, a study has found.', 'Here is how they fare in the top three mental woes compared with older adults\\nHigh expectations and hard work are the hallmarks of success, especially in a society forged from the blood, sweat and tears of immigrants.', 'But for one in seven young people, the pressure may be too much to bear and could spell disaster in combination with other factors.', 'That is how many fall prey to mental illness by the age of 30, according to the 2010 $6.9 million Singapore Mental Health Study, whose results were released late last year.', 'Researchers expected those in this group who are in the prime of their lives to be the hardest hit, as other research has borne out how the young are most vulnerable to illnesses such as depression, obsessive compulsive behaviour and alcohol abuse.', 'They found people aged 18 to 29 to be 1.5 times more likely to suffer a mental illness than someone over 30.', 'But what surprised them was that people here start showing symptoms of certain mental conditions at a much younger age than their counterparts in other countries.', 'According to the survey of 6,600 people aged 18 and over, including both Singapore citizens and permanent residents, the median age of the onset of major depressive disorder was 26 and 20 for generalised anxiety disorder.', 'In countries such as the United States, Germany and New Zealand, the median age of onset for both conditions is at least 30 years old.', 'Without further research, one can only speculate why Singaporeans seem to be more vulnerable at an earlier age.', \"But principal investigator Chong Siow Ann notes that late adolescence and young adulthood are crucial periods in a young person's life when he has to make important decisions about almost every aspect of life.\", \"'He has to think about further studies, starting a career, forming romantic attachments and becoming independent of his parents,' said Associate Professor Chong, who is vice-chairman of the medical board (research) at the Institute of Mental Health (IMH).\", 'Dr Jasmine Pang, a senior clinical psychologist at Changi General Hospital (CGH), who works closely with young people, said people tend to be vulnerable to mental health problems when they experience such changes in their lives.', 'Eyan, 23, knows well how a stressful change in life can tip the balance.', 'The happy-go-lucky teenager, an only child who never used to give his parents any problems at home or in school, found it hard to deal with the demands of military life when he entered national service at the age of 19.', \"He said: 'I felt stressed when I was scolded for not doing something well.\", 'But nobody seemed to care about how I felt.', \"I felt like my identity was reduced to a four-digit number.'\", 'Eyan, who declined to give his full name, became depressed and started hearing voices that told him to end his life.', 'Cultural differences may well account for the earlier onset of depression and anxiety in Singaporeans, in that the society places a strong emphasis on excellence and meritocracy, and rewards people based on their merits or achievements, said Dr Pang.', \"She said: 'People learn from a young age that they should be hard-working and perform well in their studies and work.\", \"When they do not meet the high demands they place on themselves, they tend to become anxious and stressed.'\", 'The pressure young people here place on themselves is exacerbated by their caring about what other people think, especially their parents, teachers and bosses.', 'This is something which may not matter as much to young people in the West.', \"Said Associate Professor Daniel Fung, chairman of the medical board at IMH and a senior consultant at the hospital's department of child and adolescent psychiatry: 'When they do not meet the expectations of important people in their lives, they become stressed and, over time, may develop symptoms of anxiety and depression.'\", 'Eyan was fortunate in that he was familiar with mental illness.', 'At first, he would not confide in his parents, as they were busy with work.', 'But two of his uncles had schizophrenia.', 'Terrified he would go the same way, he went to his parents, who took him to a doctor at IMH.', 'SUFFERERS SEEK HELP TOO LATE\\nMost sufferers are not as well-informed as Eyan.', 'According to the study, two in three never seek help.', 'Of those who do, it can take years before they go for treatment.', 'For instance, half of those found to have a major depressive disorder waited four years before they sought treatment.', 'Those with bipolar and obsessive compulsive disorders?', 'They waited, on average, nine.', 'This has serious implications, both for the individual and the people around him.', 'For the individual, early treatment is crucial, as research shows it helps to prevent relapses, said Prof Fung.', \"'The more relapses a person has, the higher his chance of getting another one, and the less likely he is to recover from his mental illness,' he noted.\", 'Delaying treatment can be especially detrimental for young people, as it could mean they never get a head start in life.', \"Said CGH's Dr Pang: 'The illness can affect their ability to make friends, find a partner or a job.\", \"It can also affect their ability to contribute to society.'\", 'On a wider scale, mental illness imposes the greatest burden on society.', 'more than any other illness which strikes people aged 15 to 44, a study by the Health Ministry in 2004 found.', 'This includes the loss to the economy because people are unable to contribute to it as well as the costs of seeking treatment.', 'The urgency of the problem was underscored by other findings of the study.', \"A young person's risk of alcohol dependence and bipolar disorder is 3.6 and 3.3 times higher than those aged 30 years and above.\", 'Those who seek help will find there are now more resources available to them.', 'Provisions to combat mental illness in the young have been written into the National Mental Health Blueprint, which was introduced in 2007 to promote mental health here.', 'The Government is funding youth programmes by the Institute of Mental Health, such as the Response, Early Intervention and Assessment in Community Mental Health and Community Health Assessment Team.', \"Eyan was in IMH's Support for Wellness Achievement Programme, where he met a case manager regularly to identify the triggers for his depression and learn how to cope with them.\", 'He also volunteered for a research project by IMH to understand what makes some young people more likely to develop a mental disorder than others.', \"Now 23, and working as a health-care professional, Eyan said: 'I would advise young people who show signs of mental disorder to seek the help of a professional early.\", \"Don't let the stigma of seeking treatment hold you back.\", \"'I was lucky to have sought treatment early because I saw how schizophrenia had affected the lives of my two uncles.\", \"I didn't want the same to happen to me.'\", 'leawee@sph.com.sg\\nMajor depressive disorder\\n1 in 14 young people has it\\n1 in 19 for those aged 30 & above\\nObsessive compulsive disorder\\n1 in 24 young people has it\\n1 in 37 for those aged 30 & above\\nAlcohol abuse\\n1 in 33 young people has it\\n1 in 32 for those aged 30 & above\\nIllustration Caption: -- ST FILE PHOTO']\n"
     ]
    }
   ],
   "source": [
    "print(subset['sents'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function DependencyGraph.__init__.<locals>.<lambda> at 0x118c3d7b8>,\n",
      "            {0: {'address': 0,\n",
      "                 'ctag': 'TOP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'root': [2]}),\n",
      "                 'feats': None,\n",
      "                 'head': None,\n",
      "                 'lemma': None,\n",
      "                 'rel': None,\n",
      "                 'tag': 'TOP',\n",
      "                 'word': None},\n",
      "             1: {'address': 1,\n",
      "                 'ctag': 'RB',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 2,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'advmod',\n",
      "                 'tag': 'RB',\n",
      "                 'word': 'Here'},\n",
      "             2: {'address': 2,\n",
      "                 'ctag': 'VBZ',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'advmod': [1],\n",
      "                                      'dep': [5]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 0,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'root',\n",
      "                 'tag': 'VBZ',\n",
      "                 'word': 'is'},\n",
      "             3: {'address': 3,\n",
      "                 'ctag': 'WRB',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 5,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'advmod',\n",
      "                 'tag': 'WRB',\n",
      "                 'word': 'how'},\n",
      "             4: {'address': 4,\n",
      "                 'ctag': 'PRP',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 5,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'nsubj',\n",
      "                 'tag': 'PRP',\n",
      "                 'word': 'they'},\n",
      "             5: {'address': 5,\n",
      "                 'ctag': 'VBP',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'advmod': [3],\n",
      "                                      'nmod': [11],\n",
      "                                      'nsubj': [4]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 2,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'dep',\n",
      "                 'tag': 'VBP',\n",
      "                 'word': 'fare'},\n",
      "             6: {'address': 6,\n",
      "                 'ctag': 'IN',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 11,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'case',\n",
      "                 'tag': 'IN',\n",
      "                 'word': 'in'},\n",
      "             7: {'address': 7,\n",
      "                 'ctag': 'DT',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 11,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'det',\n",
      "                 'tag': 'DT',\n",
      "                 'word': 'the'},\n",
      "             8: {'address': 8,\n",
      "                 'ctag': 'JJ',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 11,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'amod',\n",
      "                 'tag': 'JJ',\n",
      "                 'word': 'top'},\n",
      "             9: {'address': 9,\n",
      "                 'ctag': 'CD',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 11,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'nummod',\n",
      "                 'tag': 'CD',\n",
      "                 'word': 'three'},\n",
      "             10: {'address': 10,\n",
      "                  'ctag': 'JJ',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 11,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'amod',\n",
      "                  'tag': 'JJ',\n",
      "                  'word': 'mental'},\n",
      "             11: {'address': 11,\n",
      "                  'ctag': 'NNS',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'amod': [8, 10],\n",
      "                                       'case': [6],\n",
      "                                       'det': [7],\n",
      "                                       'nmod': [15],\n",
      "                                       'nummod': [9]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 5,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'nmod',\n",
      "                  'tag': 'NNS',\n",
      "                  'word': 'woes'},\n",
      "             12: {'address': 12,\n",
      "                  'ctag': 'VBN',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 15,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'case',\n",
      "                  'tag': 'VBN',\n",
      "                  'word': 'compared'},\n",
      "             13: {'address': 13,\n",
      "                  'ctag': 'IN',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 15,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'case',\n",
      "                  'tag': 'IN',\n",
      "                  'word': 'with'},\n",
      "             14: {'address': 14,\n",
      "                  'ctag': 'JJR',\n",
      "                  'deps': defaultdict(<class 'list'>, {}),\n",
      "                  'feats': '_',\n",
      "                  'head': 15,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'amod',\n",
      "                  'tag': 'JJR',\n",
      "                  'word': 'older'},\n",
      "             15: {'address': 15,\n",
      "                  'ctag': 'NNS',\n",
      "                  'deps': defaultdict(<class 'list'>,\n",
      "                                      {'amod': [14],\n",
      "                                       'case': [12, 13]}),\n",
      "                  'feats': '_',\n",
      "                  'head': 11,\n",
      "                  'lemma': '_',\n",
      "                  'rel': 'nmod',\n",
      "                  'tag': 'NNS',\n",
      "                  'word': 'adults'}})\n"
     ]
    }
   ],
   "source": [
    "depParses = list(stanford.depParser.raw_parse_sents(list(subset['sents'][0])))\n",
    "secondSentDepParseTree = list(depParses[1])[0]\n",
    "print(secondSentDepParseTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"567pt\" height=\"474pt\"\n",
       " viewBox=\"0.00 0.00 566.79 474.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 470)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-470 562.7949,-470 562.7949,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"81\" y=\"-443.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"81\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2 (is)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M81,-429.7616C81,-418.3597 81,-403.4342 81,-390.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"84.5001,-390.2121 81,-380.2121 77.5001,-390.2121 84.5001,-390.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"92.2759\" y=\"-400.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 (Here)</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>2&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M63.7109,-343.8018C59.5121,-338.4386 55.4896,-332.3066 52.8965,-326 50.1272,-319.265 48.5358,-311.6182 47.6491,-304.3662\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"51.1183,-303.8505 46.7988,-294.1762 44.1425,-304.4326 51.1183,-303.8505\"/>\n",
       "<text text-anchor=\"middle\" x=\"75.5518\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"125\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">5 (fare)</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;5 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M90.3313,-343.7616C96.3398,-332.0176 104.2609,-316.5355 111.0147,-303.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"114.2432,-304.7088 115.6822,-294.2121 108.0115,-301.5204 114.2432,-304.7088\"/>\n",
       "<text text-anchor=\"middle\" x=\"116.1069\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">dep</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3 (how)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;3 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>5&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M96.3162,-258.036C89.1498,-252.7583 81.8251,-246.6101 75.8965,-240 69.834,-233.2407 64.4446,-225.0064 60.0107,-217.1866\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"63.0653,-215.4766 55.2616,-208.3105 56.8932,-218.779 63.0653,-215.4766\"/>\n",
       "<text text-anchor=\"middle\" x=\"98.5518\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"125\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (they)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;4 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>5&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M125,-257.7616C125,-246.3597 125,-231.4342 125,-218.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"128.5001,-218.2121 125,-208.2121 121.5001,-218.2121 128.5001,-218.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"140.1689\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>11</title>\n",
       "<text text-anchor=\"middle\" x=\"209\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">11 (woes)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;11 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M142.8142,-257.7616C154.842,-245.4475 170.884,-229.0235 184.1621,-215.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"186.7278,-217.8115 191.2114,-208.2121 181.7202,-212.9203 186.7278,-217.8115\"/>\n",
       "<text text-anchor=\"middle\" x=\"188.9448\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">6 (in)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;6 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>11&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M173.3287,-175.6899C149.7084,-166.197 122.5906,-155.2582 119.9102,-154 100.8826,-145.0686 80.2453,-134.098 63.2645,-124.7108\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"64.6792,-121.4925 54.2403,-119.6816 61.2715,-127.607 64.6792,-121.4925\"/>\n",
       "<text text-anchor=\"middle\" x=\"132.0449\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">7 (the)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;7 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>11&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M185.9404,-171.9716C169.8067,-159.358 148.0616,-142.3573 130.3549,-128.5139\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"132.0631,-125.4066 122.0292,-122.0047 127.7516,-130.9213 132.0631,-125.4066\"/>\n",
       "<text text-anchor=\"middle\" x=\"169.5518\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"171\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">8 (top)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;8 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>11&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M200.9412,-171.7616C195.8023,-160.1316 189.0437,-144.8357 183.2484,-131.72\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"186.2903,-129.9444 179.0472,-122.2121 179.8875,-132.7736 186.2903,-129.9444\"/>\n",
       "<text text-anchor=\"middle\" x=\"208.5518\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">amod</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"248\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">9 (three)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;9 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>11&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M217.743,-171.7298C220.4241,-166.0623 223.3616,-159.7857 226,-154 229.3004,-146.7626 232.7997,-138.9003 236.0133,-131.5996\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"239.3665,-132.6677 240.173,-122.1036 232.9547,-129.859 239.3665,-132.6677\"/>\n",
       "<text text-anchor=\"middle\" x=\"257.8896\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nummod</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"338\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">10 (mental)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>11&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M244.5532,-176.6522C258.378,-170.6933 273.9874,-162.9781 287,-154 297.104,-147.0287 307.0203,-137.9717 315.383,-129.4847\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"317.9252,-131.8904 322.2978,-122.24 312.8615,-127.0573 317.9252,-131.8904\"/>\n",
       "<text text-anchor=\"middle\" x=\"319.5518\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">amod</text>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>15</title>\n",
       "<text text-anchor=\"middle\" x=\"435\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">15 (adults)</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;15 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>11&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M244.6916,-182.2566C271.1864,-175.9976 307.9492,-166.2283 339,-154 358.031,-146.5053 378.2331,-136.3451 395.1703,-127.1409\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"397.1517,-130.045 404.2157,-122.1487 393.7693,-123.9164 397.1517,-130.045\"/>\n",
       "<text text-anchor=\"middle\" x=\"384.9448\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nmod</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"334\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">12 (compared)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;12 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>15&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M413.5805,-85.7616C398.9847,-73.3335 379.4724,-56.719 363.422,-43.0524\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"365.2715,-40.0303 355.3886,-36.2121 360.7334,-45.36 365.2715,-40.0303\"/>\n",
       "<text text-anchor=\"middle\" x=\"403.0449\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>13</title>\n",
       "<text text-anchor=\"middle\" x=\"435\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">13 (with)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;13 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>15&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M435,-85.7616C435,-74.3597 435,-59.4342 435,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"438.5001,-46.2121 435,-36.2121 431.5001,-46.2121 438.5001,-46.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"447.0449\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">case</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>14</title>\n",
       "<text text-anchor=\"middle\" x=\"523\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">14 (older)</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;14 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>15&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M453.6625,-85.7616C466.263,-73.4475 483.0689,-57.0235 496.9793,-43.4293\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"499.6587,-45.7046 504.3644,-36.2121 494.7662,-40.6983 499.6587,-45.7046\"/>\n",
       "<text text-anchor=\"middle\" x=\"499.5518\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">amod</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x119e9d2e8>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphviz.Source(secondSentDepParseTree.to_dot())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information extraction\n",
    "\n",
    "Information extraction approaches typically (as here, with Stanford's Open IE engine) ride atop the dependency parse of a sentence. They are a pre-coded example of the type analyzed in the prior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OpenIE run\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.8 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... \n",
      "[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 19.34 (s)\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [21.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator natlog\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator openie\n",
      "[main] INFO edu.stanford.nlp.naturalli.ClauseSplitter - Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [0.0188 seconds]\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - Processing file: /var/folders/pl/wb_8ggm95nv3rcznnv_9lkww0000gn/T/tmph7vnccq0\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - All files have been queued; awaiting termination...\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - DONE processing files. 0 exceptions encountered.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ieDF = stanford.openIE(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`openIE()` prints everything stanford core produces and we can see from looking at it that initializing the dependency parser takes most of the time, so calling the function will always take at least 12 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I saw the elephant in my pajamas.',\n",
       " 'The quick brown fox jumped over the lazy dog.',\n",
       " 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.',\n",
       " 'Trayvon Benjamin Martin was an African American from Miami Gardens, Florida, who, at 17 years old, was fatally shot by George Zimmerman, a neighborhood watch volunteer, in Sanford, Florida.',\n",
       " 'Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certainty</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>elephant</td>\n",
       "      <td>is in</td>\n",
       "      <td>my pajamas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I</td>\n",
       "      <td>saw</td>\n",
       "      <td>elephant in my pajamas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I</td>\n",
       "      <td>saw</td>\n",
       "      <td>elephant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>brown fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>quick fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>fox</td>\n",
       "      <td>jumped over</td>\n",
       "      <td>lazy dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in interview with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in recent interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed stimulus efforts in</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in recent intervie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in recent interview with Wall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in recent interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>short-term stimulus efforts</td>\n",
       "      <td>is in</td>\n",
       "      <td>recent interview with Wall Street Journal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts in interview with Wall Street...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>stimulus efforts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>discussed</td>\n",
       "      <td>short-term stimulus efforts in interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>recent interview</td>\n",
       "      <td>is with</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Martin</td>\n",
       "      <td>was</td>\n",
       "      <td>African</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was African American from</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was American from</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was</td>\n",
       "      <td>American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Trayvon Benjamin Martin</td>\n",
       "      <td>was</td>\n",
       "      <td>African American</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    certainty                      subject                           verb  \\\n",
       "0         1.0                     elephant                          is in   \n",
       "1         1.0                            I                            saw   \n",
       "2         1.0                            I                            saw   \n",
       "3         1.0              quick brown fox                    jumped over   \n",
       "4         1.0              quick brown fox                    jumped over   \n",
       "5         1.0                    quick fox                    jumped over   \n",
       "6         1.0                          fox                    jumped over   \n",
       "7         1.0                    brown fox                    jumped over   \n",
       "8         1.0                    brown fox                    jumped over   \n",
       "9         1.0                    quick fox                    jumped over   \n",
       "10        1.0                          fox                    jumped over   \n",
       "11        1.0            Christine Lagarde                      discussed   \n",
       "12        1.0            Christine Lagarde                      discussed   \n",
       "13        1.0            Christine Lagarde                      discussed   \n",
       "14        1.0            Christine Lagarde  discussed stimulus efforts in   \n",
       "15        1.0            Christine Lagarde                      discussed   \n",
       "16        1.0            Christine Lagarde                      discussed   \n",
       "17        1.0            Christine Lagarde                      discussed   \n",
       "18        1.0  short-term stimulus efforts                          is in   \n",
       "19        1.0            Christine Lagarde                      discussed   \n",
       "20        1.0            Christine Lagarde                      discussed   \n",
       "21        1.0            Christine Lagarde                      discussed   \n",
       "22        1.0            Christine Lagarde                      discussed   \n",
       "23        1.0             recent interview                        is with   \n",
       "24        1.0                       Martin                            was   \n",
       "25        1.0      Trayvon Benjamin Martin      was African American from   \n",
       "26        1.0      Trayvon Benjamin Martin              was American from   \n",
       "27        1.0      Trayvon Benjamin Martin                            was   \n",
       "28        1.0      Trayvon Benjamin Martin                            was   \n",
       "\n",
       "                                               object  \n",
       "0                                          my pajamas  \n",
       "1                              elephant in my pajamas  \n",
       "2                                            elephant  \n",
       "3                                            lazy dog  \n",
       "4                                                 dog  \n",
       "5                                                 dog  \n",
       "6                                                 dog  \n",
       "7                                            lazy dog  \n",
       "8                                                 dog  \n",
       "9                                            lazy dog  \n",
       "10                                           lazy dog  \n",
       "11  short-term stimulus efforts in interview with ...  \n",
       "12                      stimulus efforts in interview  \n",
       "13               stimulus efforts in recent interview  \n",
       "14                                             France  \n",
       "15  short-term stimulus efforts in recent intervie...  \n",
       "16  stimulus efforts in recent interview with Wall...  \n",
       "17    short-term stimulus efforts in recent interview  \n",
       "18          recent interview with Wall Street Journal  \n",
       "19  stimulus efforts in interview with Wall Street...  \n",
       "20                                   stimulus efforts  \n",
       "21                        short-term stimulus efforts  \n",
       "22           short-term stimulus efforts in interview  \n",
       "23                                Wall Street Journal  \n",
       "24                                            African  \n",
       "25                                            Florida  \n",
       "26                                            Florida  \n",
       "27                                           American  \n",
       "28                                   African American  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No buffalos (because there were no verbs), but the rest is somewhat promising. Note, however, that it abandoned the key theme of the sentence about the tragic Trayvon Martin death (\"fatally shot\"), likely because it was buried so deeply within the complex phrase structure. This is obviously a challenge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 4*</span>\n",
    "\n",
    "<span style=\"color:red\">How would you extract relevant information about the Trayvon Martin sentence directly from the dependency parse (above)? Code an example here. (For instance, what compound nouns show up with what verb phrases within the sentence?) How could these approaches inform your research project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27            American\n",
       "28    African American\n",
       "Name: object, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# match verbs correspond with the subject of interest (Trayvon Martin)\n",
    "ieDF[(ieDF['subject'] == 'Trayvon Benjamin Martin') & (ieDF['verb'] == 'was')]['object']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Well, this would allow me to extract key SVO tuples from my text: identify common arguments like \"X is Y\", and find the set of Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also look for subject, object, target triples in one of the reddit stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OpenIE run\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [2.0 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... \n",
      "[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 19.395 (s)\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [22.0 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator natlog\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator openie\n",
      "[main] INFO edu.stanford.nlp.naturalli.ClauseSplitter - Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [0.0161 seconds]\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - Processing file: /var/folders/pl/wb_8ggm95nv3rcznnv_9lkww0000gn/T/tmpekjbf7zs\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - All files have been queued; awaiting termination...\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - DONE processing files. 0 exceptions encountered.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ieDF = stanford.openIE(redditTopScores['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certainty</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>'ll get</td>\n",
       "      <td>calls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>Quite often 'll get</td>\n",
       "      <td>calls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>often 'll get</td>\n",
       "      <td>calls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.831036</td>\n",
       "      <td>we</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>straight analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>straight analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>would supply analog cable to</td>\n",
       "      <td>homes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.831036</td>\n",
       "      <td>we</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.831036</td>\n",
       "      <td>we</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>straight analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct from wall to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>straight analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.831036</td>\n",
       "      <td>we</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct to TV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>we</td>\n",
       "      <td>would supply analog cable to</td>\n",
       "      <td>many homes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.774359</td>\n",
       "      <td>analog cable</td>\n",
       "      <td>coax</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>our equipment</td>\n",
       "      <td>receive</td>\n",
       "      <td>channels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>our digital equipment</td>\n",
       "      <td>receive</td>\n",
       "      <td>channels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>repeat offenders</td>\n",
       "      <td>is with</td>\n",
       "      <td>long ticket histories of types of issues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>anyway get</td>\n",
       "      <td>call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>get</td>\n",
       "      <td>call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>he</td>\n",
       "      <td>speak with</td>\n",
       "      <td>machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>So anyway get</td>\n",
       "      <td>call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>he</td>\n",
       "      <td>has</td>\n",
       "      <td>speak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>he</td>\n",
       "      <td>has</td>\n",
       "      <td>speak with machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>call</td>\n",
       "      <td>however was going</td>\n",
       "      <td>little different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>call</td>\n",
       "      <td>was going</td>\n",
       "      <td>little different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.780294</td>\n",
       "      <td>handling</td>\n",
       "      <td>types of</td>\n",
       "      <td>customers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>call</td>\n",
       "      <td>was going</td>\n",
       "      <td>different</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>man happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>man happy again for once in long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>old man happy for once in time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>man</td>\n",
       "      <td>happy for</td>\n",
       "      <td>once</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>old man</td>\n",
       "      <td>happy again for</td>\n",
       "      <td>once in time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>man</td>\n",
       "      <td>happy again for</td>\n",
       "      <td>once</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>old man happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>old man</td>\n",
       "      <td>happy again for</td>\n",
       "      <td>once in very long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>man happy again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>man</td>\n",
       "      <td>happy for</td>\n",
       "      <td>once in long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>old man happy again for once in time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>man happy again for once</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>old man happy again for once in very long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>old man happy again for once in long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>old man</td>\n",
       "      <td>happy for</td>\n",
       "      <td>once in time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>man happy for once in time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>man happy for once in very long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>old man happy for once in very long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>really made</td>\n",
       "      <td>man happy again for once in long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>it</td>\n",
       "      <td>made</td>\n",
       "      <td>man happy again for once in very long time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>letter</td>\n",
       "      <td>put on</td>\n",
       "      <td>our front entrance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>letter</td>\n",
       "      <td>put to</td>\n",
       "      <td>retail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>letter</td>\n",
       "      <td>put on</td>\n",
       "      <td>our entrance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>letter</td>\n",
       "      <td>was</td>\n",
       "      <td>framed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>they</td>\n",
       "      <td>going through</td>\n",
       "      <td>lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>they</td>\n",
       "      <td>just going through</td>\n",
       "      <td>lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>still think about</td>\n",
       "      <td>Mr. Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>still think occasionally about</td>\n",
       "      <td>Mr. Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>think occasionally about</td>\n",
       "      <td>Mr. Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>I</td>\n",
       "      <td>think about</td>\n",
       "      <td>Mr. Smith</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     certainty                subject                            verb  \\\n",
       "0     1.000000                     we                         'll get   \n",
       "1     1.000000                     we             Quite often 'll get   \n",
       "2     1.000000                     we                   often 'll get   \n",
       "3     0.831036                     we                            coax   \n",
       "4     0.774359  straight analog cable                            coax   \n",
       "5     0.774359           analog cable                            coax   \n",
       "6     0.774359  straight analog cable                            coax   \n",
       "7     1.000000                     we    would supply analog cable to   \n",
       "8     0.831036                     we                            coax   \n",
       "9     0.774359           analog cable                            coax   \n",
       "10    0.831036                     we                            coax   \n",
       "11    0.774359  straight analog cable                            coax   \n",
       "12    0.774359  straight analog cable                            coax   \n",
       "13    0.831036                     we                            coax   \n",
       "14    0.774359           analog cable                            coax   \n",
       "15    1.000000                     we    would supply analog cable to   \n",
       "16    0.774359           analog cable                            coax   \n",
       "17    1.000000          our equipment                         receive   \n",
       "18    1.000000  our digital equipment                         receive   \n",
       "19    1.000000       repeat offenders                         is with   \n",
       "20    1.000000                      I                      anyway get   \n",
       "21    1.000000                      I                             get   \n",
       "22    1.000000                     he                      speak with   \n",
       "23    1.000000                      I                   So anyway get   \n",
       "24    1.000000                     he                             has   \n",
       "25    1.000000                     he                             has   \n",
       "26    1.000000                   call               however was going   \n",
       "27    1.000000                   call                       was going   \n",
       "28    0.780294               handling                        types of   \n",
       "29    1.000000                   call                       was going   \n",
       "..         ...                    ...                             ...   \n",
       "161   1.000000                     it                            made   \n",
       "162   1.000000                     it                            made   \n",
       "163   1.000000                     it                     really made   \n",
       "164   1.000000                    man                       happy for   \n",
       "165   1.000000                old man                 happy again for   \n",
       "166   1.000000                    man                 happy again for   \n",
       "167   1.000000                     it                     really made   \n",
       "168   1.000000                old man                 happy again for   \n",
       "169   1.000000                     it                     really made   \n",
       "170   1.000000                    man                       happy for   \n",
       "171   1.000000                     it                            made   \n",
       "172   1.000000                     it                            made   \n",
       "173   1.000000                     it                     really made   \n",
       "174   1.000000                     it                     really made   \n",
       "175   1.000000                old man                       happy for   \n",
       "176   1.000000                     it                     really made   \n",
       "177   1.000000                     it                            made   \n",
       "178   1.000000                     it                            made   \n",
       "179   1.000000                     it                     really made   \n",
       "180   1.000000                     it                            made   \n",
       "181   1.000000                 letter                          put on   \n",
       "182   1.000000                 letter                          put to   \n",
       "183   1.000000                 letter                          put on   \n",
       "184   1.000000                 letter                             was   \n",
       "185   1.000000                   they                   going through   \n",
       "186   1.000000                   they              just going through   \n",
       "187   1.000000                      I               still think about   \n",
       "188   1.000000                      I  still think occasionally about   \n",
       "189   1.000000                      I        think occasionally about   \n",
       "190   1.000000                      I                     think about   \n",
       "\n",
       "                                             object  \n",
       "0                                             calls  \n",
       "1                                             calls  \n",
       "2                                             calls  \n",
       "3                                      direct to TV  \n",
       "4                                  direct from wall  \n",
       "5                            direct from wall to TV  \n",
       "6                                      direct to TV  \n",
       "7                                             homes  \n",
       "8                                  direct from wall  \n",
       "9                                  direct from wall  \n",
       "10                           direct from wall to TV  \n",
       "11                           direct from wall to TV  \n",
       "12                                           direct  \n",
       "13                                           direct  \n",
       "14                                     direct to TV  \n",
       "15                                       many homes  \n",
       "16                                           direct  \n",
       "17                                         channels  \n",
       "18                                         channels  \n",
       "19         long ticket histories of types of issues  \n",
       "20                                             call  \n",
       "21                                             call  \n",
       "22                                          machine  \n",
       "23                                             call  \n",
       "24                                            speak  \n",
       "25                               speak with machine  \n",
       "26                                 little different  \n",
       "27                                 little different  \n",
       "28                                        customers  \n",
       "29                                        different  \n",
       "..                                              ...  \n",
       "161                                       man happy  \n",
       "162           man happy again for once in long time  \n",
       "163                  old man happy for once in time  \n",
       "164                                            once  \n",
       "165                                    once in time  \n",
       "166                                            once  \n",
       "167                                   old man happy  \n",
       "168                          once in very long time  \n",
       "169                                 man happy again  \n",
       "170                               once in long time  \n",
       "171            old man happy again for once in time  \n",
       "172                        man happy again for once  \n",
       "173  old man happy again for once in very long time  \n",
       "174       old man happy again for once in long time  \n",
       "175                                    once in time  \n",
       "176                      man happy for once in time  \n",
       "177            man happy for once in very long time  \n",
       "178        old man happy for once in very long time  \n",
       "179           man happy again for once in long time  \n",
       "180      man happy again for once in very long time  \n",
       "181                              our front entrance  \n",
       "182                                          retail  \n",
       "183                                    our entrance  \n",
       "184                                          framed  \n",
       "185                                             lot  \n",
       "186                                             lot  \n",
       "187                                       Mr. Smith  \n",
       "188                                       Mr. Smith  \n",
       "189                                       Mr. Smith  \n",
       "190                                       Mr. Smith  \n",
       "\n",
       "[191 rows x 4 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's almost 200 triples in only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(redditTopScores['sentences'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentences and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "971"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(s) for s in redditTopScores['sentences'][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find at the most common subject in this story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I                        48\n",
       "it                       42\n",
       "he                       19\n",
       "He                       18\n",
       "we                       11\n",
       "old man                   8\n",
       "man                       8\n",
       "letter                    4\n",
       "call                      4\n",
       "our booking calendar      4\n",
       "straight analog cable     4\n",
       "analog cable              4\n",
       "my supervisor             3\n",
       "his TV set                2\n",
       "they                      2\n",
       "TV                        2\n",
       "you                       2\n",
       "our digital equipment     1\n",
       "me                        1\n",
       "people                    1\n",
       "repeat offenders          1\n",
       "our equipment             1\n",
       "handling                  1\n",
       "Name: subject, dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I is followed by various male pronouns and compound nouns (e.g., \"old man\"). 'I' occures most often with the following verbs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "could come                        8\n",
       "even brought                      5\n",
       "brought                           5\n",
       "had                               4\n",
       "was                               4\n",
       "speak for                         3\n",
       "had cable within                  1\n",
       "think occasionally about          1\n",
       "felt                              1\n",
       "do                                1\n",
       "instantly felt                    1\n",
       "get to                            1\n",
       "ask                               1\n",
       "'ve dealt with                    1\n",
       "have                              1\n",
       "anyway get                        1\n",
       "took                              1\n",
       "think about                       1\n",
       "get                               1\n",
       "eventually had                    1\n",
       "still think about                 1\n",
       "speak with                        1\n",
       "So anyway get                     1\n",
       "complaint in                      1\n",
       "still think occasionally about    1\n",
       "Name: verb, dtype: int64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF[ieDF['subject'] == 'I']['verb'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the following objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mr. Smith                                             4\n",
       "call                                                  3\n",
       "him                                                   3\n",
       "simplified remote                                     2\n",
       "willing                                               2\n",
       "bad                                                   2\n",
       "remote for his set top box                            2\n",
       "remote                                                2\n",
       "this                                                  2\n",
       "simplified remote for his set top box                 2\n",
       "get                                                   2\n",
       "speak with her for bit about account                  1\n",
       "speak for bit about account for Mr. Smith             1\n",
       "book                                                  1\n",
       "speak for bit about account                           1\n",
       "experience                                            1\n",
       "speak with her                                        1\n",
       "30 seconds                                            1\n",
       "her                                                   1\n",
       "bit                                                   1\n",
       "bit about account                                     1\n",
       "speak with her for bit about account for Mr. Smith    1\n",
       "speak for bit                                         1\n",
       "speak                                                 1\n",
       "residence                                             1\n",
       "how useless                                           1\n",
       "cable                                                 1\n",
       "cable running                                         1\n",
       "speak with her for bit                                1\n",
       "bit about account for Mr. Smith                       1\n",
       "it                                                    1\n",
       "useless                                               1\n",
       "cable running again                                   1\n",
       "Name: object, dtype: int64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieDF[ieDF['subject'] == 'I']['object'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run the corenlp server. When you run this server (with the command below), you can click on the browswer link provided to experiment with it. Note that when we run the server, executing the command below, it interrupts the current jupyter process and you will not be able to run code here again (processes will \"hang\" and never finish) until you interrup the process by clicking \"Kernel\" and then \"Interrupt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server on http://localhost:16432 , please wait a few seconds\n",
      "click Kernel -> Then Interupt to stop            ()))             \n",
      "Exiting ()\n"
     ]
    }
   ],
   "source": [
    "stanford.startCoreServer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <span style=\"color:red\">*Exercise 5*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, perform open information extraction on a modest subset of texts relevant to your final project. Analyze the relative attachment of several subjects relative to verbs and objects and visa versa. Describe how you would select among these statements to create a database of high-value statements for your project and then do it by extracting relevant statements into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting OpenIE run\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [2.6 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... \n",
      "[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 25.688 (s)\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [29.2 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator natlog\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator openie\n",
      "[main] INFO edu.stanford.nlp.naturalli.ClauseSplitter - Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [0.0171 seconds]\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - Processing file: /var/folders/pl/wb_8ggm95nv3rcznnv_9lkww0000gn/T/tmp0iu__0z1\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - All files have been queued; awaiting termination...\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - DONE processing files. 0 exceptions encountered.\n",
      "\n",
      "Starting OpenIE run\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.9 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... \n",
      "[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 22.126 (s)\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [24.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator natlog\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator openie\n",
      "[main] INFO edu.stanford.nlp.naturalli.ClauseSplitter - Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [0.0208 seconds]\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - Processing file: /var/folders/pl/wb_8ggm95nv3rcznnv_9lkww0000gn/T/tmpj9plkbei\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - All files have been queued; awaiting termination...\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - DONE processing files. 0 exceptions encountered.\n",
      "\n",
      "Starting OpenIE run\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [2.1 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... \n",
      "[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 17.976 (s)\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [20.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator natlog\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator openie\n",
      "[main] INFO edu.stanford.nlp.naturalli.ClauseSplitter - Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [0.0198 seconds]\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - Processing file: /var/folders/pl/wb_8ggm95nv3rcznnv_9lkww0000gn/T/tmpp6lvuqyr\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - All files have been queued; awaiting termination...\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - DONE processing files. 0 exceptions encountered.\n",
      "\n",
      "Starting OpenIE run\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [2.7 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... \n",
      "[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 31.602 (s)\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [35.1 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator natlog\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator openie\n",
      "[main] INFO edu.stanford.nlp.naturalli.ClauseSplitter - Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [0.0199 seconds]\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - Processing file: /var/folders/pl/wb_8ggm95nv3rcznnv_9lkww0000gn/T/tmpdd6y4x2c\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - All files have been queued; awaiting termination...\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - DONE processing files. 0 exceptions encountered.\n",
      "\n",
      "Starting OpenIE run\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [2.1 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator depparse\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Loading depparse model file: edu/stanford/nlp/models/parser/nndep/english_UD.gz ... \n",
      "[main] INFO edu.stanford.nlp.parser.nndep.Classifier - PreComputed 99996, Elapsed Time: 20.829 (s)\n",
      "[main] INFO edu.stanford.nlp.parser.nndep.DependencyParser - Initializing dependency parser ... done [23.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator natlog\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator openie\n",
      "[main] INFO edu.stanford.nlp.naturalli.ClauseSplitter - Loading clause splitter from edu/stanford/nlp/models/naturalli/clauseSearcherModel.ser.gz ... done [0.0169 seconds]\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - Processing file: /var/folders/pl/wb_8ggm95nv3rcznnv_9lkww0000gn/T/tmpjm7_6jsp\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - All files have been queued; awaiting termination...\n",
      "[main] INFO edu.stanford.nlp.naturalli.OpenIE - DONE processing files. 0 exceptions encountered.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ieDF0 = stanford.openIE(subset['text'][0])\n",
    "ieDF1 = stanford.openIE(subset['text'][1])\n",
    "ieDF2 = stanford.openIE(subset['text'][2])\n",
    "ieDF3 = stanford.openIE(subset['text'][3])\n",
    "ieDF4 = stanford.openIE(subset['text'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ieDF0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#join the 5 into one main DF\n",
    "iemainDF = pandas.concat([ieDF0, ieDF1, ieDF2, ieDF3, ieDF4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is                        63\n",
       "are                       49\n",
       "is in                     47\n",
       "was                       43\n",
       "has                       42\n",
       "had                       31\n",
       "be                        30\n",
       "may pose now              18\n",
       "was among                 18\n",
       "Indeed may pose now       18\n",
       "Indeed may pose           18\n",
       "may pose                  18\n",
       "showing symptoms at       16\n",
       "used as                   16\n",
       "having                    12\n",
       "fare in                   12\n",
       "was in                    12\n",
       "develop                   10\n",
       "allowed                   10\n",
       "have                      10\n",
       "Even allowed              10\n",
       "took                       8\n",
       "have read                  8\n",
       "are crucial periods in     8\n",
       "live in                    8\n",
       "going on                   8\n",
       "might seem                 8\n",
       "are periods in             8\n",
       "imposes                    8\n",
       "found                      8\n",
       "                          ..\n",
       "has tripled since          1\n",
       "also volunteered           1\n",
       "see accountants as         1\n",
       "ended up too in            1\n",
       "do with                    1\n",
       "Come At                    1\n",
       "sculpture for              1\n",
       "plans                      1\n",
       "slept in                   1\n",
       "colleague from             1\n",
       "remember                   1\n",
       "earn fast                  1\n",
       "exercise                   1\n",
       "has known                  1\n",
       "was youngest of            1\n",
       "were like                  1\n",
       "was self-financed In       1\n",
       "looks at                   1\n",
       "waited For                 1\n",
       "doing in                   1\n",
       "deal urgently with         1\n",
       "was committed With         1\n",
       "psychologist at            1\n",
       "started                    1\n",
       "died on                    1\n",
       "acquiring                  1\n",
       "wait for                   1\n",
       "visited                    1\n",
       "offering                   1\n",
       "respect                    1\n",
       "Name: verb, Length: 419, dtype: int64"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iemainDF['verb'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "He                                      127\n",
       "he                                       98\n",
       "we                                       65\n",
       "I                                        58\n",
       "people                                   47\n",
       "It                                       38\n",
       "they                                     38\n",
       "divide                                   36\n",
       "local-foreign divide                     36\n",
       "it                                       31\n",
       "We                                       28\n",
       "Illegal immigration                      24\n",
       "immigration                              24\n",
       "same                                     24\n",
       "They                                     19\n",
       "Singapore                                18\n",
       "WARREN FERNANDEZ                         13\n",
       "writer                                   12\n",
       "Mr Chong                                 12\n",
       "number                                   10\n",
       "ideal                                    10\n",
       "pressure                                 10\n",
       "late adolescence                         10\n",
       "foreigners                               10\n",
       "adolescence                              10\n",
       "MERITOCRACY                               9\n",
       "illness                                   8\n",
       "obvious enemy                             8\n",
       "job                                       8\n",
       "young                                     8\n",
       "                                       ... \n",
       "clan associations                         1\n",
       "country 's parliamentary election         1\n",
       "most natural bonds                        1\n",
       "graduate assistant                        1\n",
       "Health Ministry                           1\n",
       "many people                               1\n",
       "jail SRI Lanka                            1\n",
       "elites                                    1\n",
       "only people                               1\n",
       "My parents                                1\n",
       "Said Associate Professor Daniel Fung      1\n",
       "firm                                      1\n",
       "divisions                                 1\n",
       "change                                    1\n",
       "institute                                 1\n",
       "fallout                                   1\n",
       "semi-detached house                       1\n",
       "EX-NATIONAL thrower James Wong 43         1\n",
       "Monica                                    1\n",
       "Chinese                                   1\n",
       "pace                                      1\n",
       "self-starter                              1\n",
       "head start                                1\n",
       "grandfather                               1\n",
       "hospital                                  1\n",
       "many problems                             1\n",
       "Mr Chong Kee Hiong                        1\n",
       "good grounding                            1\n",
       "hosted                                    1\n",
       "move                                      1\n",
       "Name: subject, Length: 255, dtype: int64"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iemainDF['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "He                                      127\n",
       "he                                       98\n",
       "we                                       65\n",
       "I                                        58\n",
       "people                                   47\n",
       "It                                       38\n",
       "they                                     38\n",
       "divide                                   36\n",
       "local-foreign divide                     36\n",
       "it                                       31\n",
       "We                                       28\n",
       "Illegal immigration                      24\n",
       "immigration                              24\n",
       "same                                     24\n",
       "They                                     19\n",
       "Singapore                                18\n",
       "WARREN FERNANDEZ                         13\n",
       "writer                                   12\n",
       "Mr Chong                                 12\n",
       "number                                   10\n",
       "ideal                                    10\n",
       "pressure                                 10\n",
       "late adolescence                         10\n",
       "foreigners                               10\n",
       "adolescence                              10\n",
       "MERITOCRACY                               9\n",
       "illness                                   8\n",
       "obvious enemy                             8\n",
       "job                                       8\n",
       "young                                     8\n",
       "                                       ... \n",
       "clan associations                         1\n",
       "country 's parliamentary election         1\n",
       "most natural bonds                        1\n",
       "graduate assistant                        1\n",
       "Health Ministry                           1\n",
       "many people                               1\n",
       "jail SRI Lanka                            1\n",
       "elites                                    1\n",
       "only people                               1\n",
       "My parents                                1\n",
       "Said Associate Professor Daniel Fung      1\n",
       "firm                                      1\n",
       "divisions                                 1\n",
       "change                                    1\n",
       "institute                                 1\n",
       "fallout                                   1\n",
       "semi-detached house                       1\n",
       "EX-NATIONAL thrower James Wong 43         1\n",
       "Monica                                    1\n",
       "Chinese                                   1\n",
       "pace                                      1\n",
       "self-starter                              1\n",
       "head start                                1\n",
       "grandfather                               1\n",
       "hospital                                  1\n",
       "many problems                             1\n",
       "Mr Chong Kee Hiong                        1\n",
       "good grounding                            1\n",
       "hosted                                    1\n",
       "move                                      1\n",
       "Name: subject, Length: 255, dtype: int64"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iemainDF['object'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "temptations       4\n",
       "elitism           1\n",
       "cousin            1\n",
       "public service    1\n",
       "Name: object, dtype: int64"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most common verb given subject\n",
    "iemainDF[iemainDF['subject'] == 'meritocracy']['object'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enemy of be            2\n",
       "obvious enemy of be    2\n",
       "has                    1\n",
       "is in                  1\n",
       "'s cousin is           1\n",
       "Name: verb, dtype: int64"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iemainDF[iemainDF['subject'] == 'meritocracy']['verb'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "brilliant students    1\n",
       "scholars              1\n",
       "dedicated scholars    1\n",
       "education             1\n",
       "students              1\n",
       "Name: subject, dtype: int64"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iemainDF[iemainDF['object'] == 'meritocracy']['subject'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "going on        4\n",
       "be basis for    1\n",
       "Name: verb, dtype: int64"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iemainDF[iemainDF['object'] == 'meritocracy']['verb'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certainty</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.0</td>\n",
       "      <td>meritocracy</td>\n",
       "      <td>is in</td>\n",
       "      <td>public service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1.0</td>\n",
       "      <td>meritocracy</td>\n",
       "      <td>enemy of be</td>\n",
       "      <td>temptations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1.0</td>\n",
       "      <td>meritocracy</td>\n",
       "      <td>obvious enemy of be</td>\n",
       "      <td>temptations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>1.0</td>\n",
       "      <td>meritocracy</td>\n",
       "      <td>enemy of be</td>\n",
       "      <td>temptations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>1.0</td>\n",
       "      <td>meritocracy</td>\n",
       "      <td>obvious enemy of be</td>\n",
       "      <td>temptations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>1.0</td>\n",
       "      <td>meritocracy</td>\n",
       "      <td>has</td>\n",
       "      <td>cousin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>1.0</td>\n",
       "      <td>meritocracy</td>\n",
       "      <td>'s cousin is</td>\n",
       "      <td>elitism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     certainty      subject                 verb          object\n",
       "39         1.0  meritocracy                is in  public service\n",
       "103        1.0  meritocracy          enemy of be     temptations\n",
       "107        1.0  meritocracy  obvious enemy of be     temptations\n",
       "202        1.0  meritocracy          enemy of be     temptations\n",
       "205        1.0  meritocracy  obvious enemy of be     temptations\n",
       "206        1.0  meritocracy                  has          cousin\n",
       "209        1.0  meritocracy         's cousin is         elitism"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iemainDF[iemainDF['subject'] == 'meritocracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certainty</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>dedicated scholars</td>\n",
       "      <td>going on</td>\n",
       "      <td>meritocracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>scholars</td>\n",
       "      <td>going on</td>\n",
       "      <td>meritocracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>students</td>\n",
       "      <td>going on</td>\n",
       "      <td>meritocracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>brilliant students</td>\n",
       "      <td>going on</td>\n",
       "      <td>meritocracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.0</td>\n",
       "      <td>education</td>\n",
       "      <td>be basis for</td>\n",
       "      <td>meritocracy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    certainty             subject          verb       object\n",
       "18        1.0  dedicated scholars      going on  meritocracy\n",
       "21        1.0            scholars      going on  meritocracy\n",
       "23        1.0            students      going on  meritocracy\n",
       "25        1.0  brilliant students      going on  meritocracy\n",
       "36        1.0           education  be basis for  meritocracy"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iemainDF[iemainDF['object'] == 'meritocracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certainty</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>meritocratic city-state</td>\n",
       "      <td>is in</td>\n",
       "      <td>place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>meritocratic city-state</td>\n",
       "      <td>is with</td>\n",
       "      <td>institutions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.0</td>\n",
       "      <td>meritocracy</td>\n",
       "      <td>is in</td>\n",
       "      <td>public service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1.0</td>\n",
       "      <td>meritocracy</td>\n",
       "      <td>enemy of be</td>\n",
       "      <td>temptations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1.0</td>\n",
       "      <td>merito-cracy</td>\n",
       "      <td>'s cousin is</td>\n",
       "      <td>elitism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1.0</td>\n",
       "      <td>meritocracy</td>\n",
       "      <td>obvious enemy of be</td>\n",
       "      <td>temptations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1.0</td>\n",
       "      <td>merito-cracy</td>\n",
       "      <td>has</td>\n",
       "      <td>cousin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>1.0</td>\n",
       "      <td>meritocracy</td>\n",
       "      <td>enemy of be</td>\n",
       "      <td>temptations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>1.0</td>\n",
       "      <td>meritocracy</td>\n",
       "      <td>obvious enemy of be</td>\n",
       "      <td>temptations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>1.0</td>\n",
       "      <td>meritocracy</td>\n",
       "      <td>has</td>\n",
       "      <td>cousin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>1.0</td>\n",
       "      <td>meritocracy</td>\n",
       "      <td>'s cousin is</td>\n",
       "      <td>elitism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     certainty                  subject                 verb          object\n",
       "12         1.0  meritocratic city-state                is in           place\n",
       "13         1.0  meritocratic city-state              is with    institutions\n",
       "39         1.0              meritocracy                is in  public service\n",
       "103        1.0              meritocracy          enemy of be     temptations\n",
       "106        1.0             merito-cracy         's cousin is         elitism\n",
       "107        1.0              meritocracy  obvious enemy of be     temptations\n",
       "111        1.0             merito-cracy                  has          cousin\n",
       "202        1.0              meritocracy          enemy of be     temptations\n",
       "205        1.0              meritocracy  obvious enemy of be     temptations\n",
       "206        1.0              meritocracy                  has          cousin\n",
       "209        1.0              meritocracy         's cousin is         elitism"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iemainDF[iemainDF['subject'].str.contains('merit')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>certainty</th>\n",
       "      <th>subject</th>\n",
       "      <th>verb</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>has</td>\n",
       "      <td>meritocratic system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>dedicated scholars</td>\n",
       "      <td>going on</td>\n",
       "      <td>meritocracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>scholars</td>\n",
       "      <td>going on</td>\n",
       "      <td>about meritocracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>brilliant students</td>\n",
       "      <td>going on</td>\n",
       "      <td>about meritocracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>scholars</td>\n",
       "      <td>going on</td>\n",
       "      <td>meritocracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>students</td>\n",
       "      <td>going on</td>\n",
       "      <td>about meritocracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>students</td>\n",
       "      <td>going on</td>\n",
       "      <td>meritocracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>dedicated scholars</td>\n",
       "      <td>going on</td>\n",
       "      <td>about meritocracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>brilliant students</td>\n",
       "      <td>going on</td>\n",
       "      <td>meritocracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>education</td>\n",
       "      <td>be basis for</td>\n",
       "      <td>meritocracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>education</td>\n",
       "      <td>be basis for</td>\n",
       "      <td>meritocracy in service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>education</td>\n",
       "      <td>be basis for</td>\n",
       "      <td>meritocracy in public service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>Confucius</td>\n",
       "      <td>was dogmatic about</td>\n",
       "      <td>possibility of meritocracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>It</td>\n",
       "      <td>needed commitments to</td>\n",
       "      <td>meritocracy ideal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.912144</td>\n",
       "      <td>institutions</td>\n",
       "      <td>vital for</td>\n",
       "      <td>spirit of meritocracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>It</td>\n",
       "      <td>needed commitments to</td>\n",
       "      <td>meritocracy ideal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     certainty             subject                   verb  \\\n",
       "19    1.000000           Singapore                    has   \n",
       "18    1.000000  dedicated scholars               going on   \n",
       "19    1.000000            scholars               going on   \n",
       "20    1.000000  brilliant students               going on   \n",
       "21    1.000000            scholars               going on   \n",
       "22    1.000000            students               going on   \n",
       "23    1.000000            students               going on   \n",
       "24    1.000000  dedicated scholars               going on   \n",
       "25    1.000000  brilliant students               going on   \n",
       "36    1.000000           education           be basis for   \n",
       "37    1.000000           education           be basis for   \n",
       "42    1.000000           education           be basis for   \n",
       "45    1.000000           Confucius     was dogmatic about   \n",
       "116   1.000000                  It  needed commitments to   \n",
       "189   0.912144        institutions              vital for   \n",
       "215   1.000000                  It  needed commitments to   \n",
       "\n",
       "                            object  \n",
       "19             meritocratic system  \n",
       "18                     meritocracy  \n",
       "19               about meritocracy  \n",
       "20               about meritocracy  \n",
       "21                     meritocracy  \n",
       "22               about meritocracy  \n",
       "23                     meritocracy  \n",
       "24               about meritocracy  \n",
       "25                     meritocracy  \n",
       "36                     meritocracy  \n",
       "37          meritocracy in service  \n",
       "42   meritocracy in public service  \n",
       "45      possibility of meritocracy  \n",
       "116              meritocracy ideal  \n",
       "189          spirit of meritocracy  \n",
       "215              meritocracy ideal  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iemainDF[iemainDF['object'].str.contains('merit')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "How would I go about selecting statements? Well, I can imagine pre-selecting a group of important keywords (meritocracy and its cognates for example). The issue I have is that some of the SVO tuples make very little sense. I'm not sure why that is the case. What the hell are the \"going on\", for instance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
